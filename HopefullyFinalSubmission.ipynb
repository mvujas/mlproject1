{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HopefullyFinalSubmission.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFPCqsatcmRU"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import data_io\n",
        "import data_preprocessing\n",
        "from implementations import *\n",
        "import validation\n",
        "import attribute_selection\n",
        "import evaluators\n",
        "import metrics\n",
        "\n",
        "# Autoreload modules\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRl8fmwZdvp4",
        "outputId": "805967f6-fdf6-45bd-ccf4-09decbc89d18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "DATA_FILE_PREFIX = '/content/drive/My Drive/mlproject1_higgs_data/'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7x1A5HTd1Md"
      },
      "source": [
        "y_train, x_train, _, cols = data_io.load_csv_data(f'{DATA_FILE_PREFIX}train.csv')\n",
        "_, x_test, ids_test, cols_train = data_io.load_csv_data(f'{DATA_FILE_PREFIX}test.csv')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQzHQa0GdxmT"
      },
      "source": [
        "col_to_index_mapping = {col_name: index - 2 for index, col_name in enumerate(cols) if index >= 2}\n",
        "y_train = (y_train + 1) // 2"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNVAcSk0pLNC"
      },
      "source": [
        "import operator\n",
        "\n",
        "def tune_lambda(y, x, grid, seed=42, history=True):\n",
        "    w_init = np.zeros(x.shape[1])\n",
        "    res = {}\n",
        "    for lambda_ in grid:\n",
        "        np.random.seed(seed)\n",
        "        train_model = lambda y_, x_: make_predictor(reg_logistic_regression_sgd(\n",
        "            y_, x_, lambda_, w_init, 5, 1000, 0.5,\n",
        "        )[0])\n",
        "        res[lambda_] = validation.cross_validation(y, x, train_model, 5)[0].mean()\n",
        "        if history:\n",
        "          print(f\"{lambda_}: {res[lambda_]:.4f}\")\n",
        "    return max(res.items(), key=operator.itemgetter(1))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8X9uAUXfLo9"
      },
      "source": [
        "def make_predictor(w):\n",
        "  def foo(features):\n",
        "    return (features @ w > 0).astype(int)\n",
        "  return foo"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDJkvPsNfz20"
      },
      "source": [
        "def train_model(y, x):\n",
        "  w_init = np.zeros(x.shape[1])\n",
        "  lambda_ = 1e-5\n",
        "  return make_predictor(reg_logistic_regression_sgd(\n",
        "      y, x, lambda_, w_init, 5, 1000, 0.5)[0])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnfLdL7cf_2h"
      },
      "source": [
        "def build_pairwise_plus(x, column_idx):\n",
        "    \"\"\"build pairwise multiplyed features x\"\"\"\n",
        "    if x.ndim == 1:\n",
        "        x = x[:, np.newaxis]\n",
        "        \n",
        "    columns = np.copy(x[:, column_idx])\n",
        "    pairwise = []\n",
        "    for i in range(columns.shape[1] - 1):\n",
        "        for j in range(i + 1, columns.shape[1] - 1):\n",
        "            pairwise.append(columns[:, i] + columns[:, j])\n",
        "    pairwise = np.array(pairwise).T\n",
        "    return np.concatenate([np.copy(x), pairwise], 1)\n",
        "\n",
        "def transformation_pipeline_median_selected(x, col_to_index_mapping=col_to_index_mapping):\n",
        "    tx = np.copy(x) # Recommended to copy x so it doesn't change\n",
        "    tx[tx == -999.] = np.nan\n",
        "    tx = data_preprocessing.apply_transformation(\n",
        "        tx,\n",
        "        [col_to_index_mapping[key] for key in col_to_index_mapping if 'PRI_jet_num' not in key],\n",
        "        data_preprocessing.standardize_with_nans,\n",
        "    )\n",
        "    # standardize and normalize may change value of fields from default missing values, so it uses matrix calculated before applying transformations\n",
        "    tx = data_preprocessing.median_missing_values(tx, np.isnan(tx)) \n",
        "    # onehot for categorical and drop one level\n",
        "    tx, col_to_index_mapping_upd = data_preprocessing.one_hot_transformation(tx, 'PRI_jet_num', col_to_index_mapping)\n",
        "    tx = tx[:, :-1]\n",
        "\n",
        "    sins = np.sin(tx)\n",
        "    coses = np.cos(tx)\n",
        "    #polys = data_preprocessing.build_poly(tx, list(range(tx.shape[1])), [2])\n",
        "    tx = np.concatenate((tx, sins, coses), axis=1)\n",
        "    \n",
        "    # add bias\n",
        "    tx = data_preprocessing.prepend_bias_column(tx)\n",
        "    first_selection_attr = [0, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 15, 16, 17, 20, 22, 25, 30, 31, 33, 34, 35, 36, 40, 41, 42, 44, 45, 47, 49, 50, 52, 57, 58, 59, 62, 63, 65, 66, 67, 68, 69, 72, 74, 75, 76, 79, 81, 82, 87, 88, 91, 94, 95, 96]\n",
        "    tx = tx[:, first_selection_attr]\n",
        "    tx = tx[:, [0, 1, 2, 3, 4, 6, 7, 8, 9, 12, 13, 15, 18, 19, 20, 22, 23, 24, 25, 27, 31, 33, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54]]\n",
        "\n",
        "    return tx\n",
        "\n",
        "def transformation_pipeline_median_selected_pairwise(x, col_to_index_mapping=col_to_index_mapping):\n",
        "    tx = np.copy(x) # Recommended to copy x so it doesn't change\n",
        "    tx[tx == -999.] = np.nan\n",
        "    tx = data_preprocessing.apply_transformation(\n",
        "        tx,\n",
        "        [col_to_index_mapping[key] for key in col_to_index_mapping if 'PRI_jet_num' not in key],\n",
        "        data_preprocessing.standardize_with_nans,\n",
        "    )\n",
        "    # standardize and normalize may change value of fields from default missing values, so it uses matrix calculated before applying transformations\n",
        "    tx = data_preprocessing.median_missing_values(tx, np.isnan(tx)) \n",
        "    # onehot for categorical and drop one level\n",
        "    tx, col_to_index_mapping_upd = data_preprocessing.one_hot_transformation(tx, 'PRI_jet_num', col_to_index_mapping)\n",
        "    tx = tx[:, :-1]\n",
        "\n",
        "    sins = np.sin(tx)\n",
        "    coses = np.cos(tx)\n",
        "    #polys = data_preprocessing.build_poly(tx, list(range(tx.shape[1])), [2])\n",
        "    tx = np.concatenate((tx, sins, coses), axis=1)\n",
        "    \n",
        "    # add bias\n",
        "    tx = data_preprocessing.prepend_bias_column(tx)\n",
        "    first_selection_attr = [0, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 15, 16, 17, 20, 22, 25, 30, 31, 33, 34, 35, 36, 40, 41, 42, 44, 45, 47, 49, 50, 52, 57, 58, 59, 62, 63, 65, 66, 67, 68, 69, 72, 74, 75, 76, 79, 81, 82, 87, 88, 91, 94, 95, 96]\n",
        "    tx = tx[:, first_selection_attr]\n",
        "    tx = tx[:, [1, 2, 3, 4, 6, 7, 8, 9, 12, 13, 15, 18, 19, 20, 22, 23, 24, 25, 27, 31, 33, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54]]\n",
        "\n",
        "    d = tx.shape[1]\n",
        "\n",
        "    tx = data_preprocessing.build_pairwise(tx, list(range(d)))\n",
        "\n",
        "    #plus = build_pairwise_plus(tx, list(range(d)))\n",
        "\n",
        "    #tx = np.concatenate((tx, mul), axis=1)\n",
        "\n",
        "    tx = data_preprocessing.prepend_bias_column(tx)\n",
        "    \n",
        "    return tx"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EP6nsaPghYg"
      },
      "source": [
        "tx_train_2 = transformation_pipeline_median_selected_pairwise(x_train)\n",
        "#top_80 = [0, 13, 30, 31, 36, 38, 39, 40, 42, 45, 52, 63, 75, 82, 84, 89, 90, 92, 101, 104, 108, 109, 128, 129, 136, 143, 166, 168, 213, 222, 224, 227, 245, 273, 290, 292, 299, 301, 303, 308, 313, 314, 351, 360, 362, 382, 427, 431, 441, 443, 444, 455, 459, 460, 464, 466, 467, 484, 491, 494, 509, 511, 557, 590, 596, 597, 663, 669, 670, 671, 693, 699, 742, 743, 745, 752, 777, 779, 780, 781]\n",
        "#tx_train_3 = tx_train_2[:, top_80]\n",
        "SELECTED = [0, 1, 8, 13, 19, 25, 28, 30, 36, 37, 38, 39, 40, 42, 43, 45, 52, 56, 63, 70, 75, 76, 78, 82, 84, 89, 90, 92, 94, 96, 98, 100, 101, 102, 104, 106, 108, 109, 111, 126, 128, 129, 130, 136, 139, 142, 143, 148, 160, 161, 165, 166, 168, 182, 213, 222, 224, 225, 227, 228, 229, 234, 240, 242, 245, 259, 261, 270, 272, 277, 290, 291, 292, 293, 295, 299, 301, 303, 308, 310, 313, 314, 316, 349, 350, 351, 354, 360, 361, 362, 364, 377, 378, 380, 382, 386, 388, 393, 394, 402, 404, 408, 409, 427, 431, 432, 433, 437, 439, 441, 442, 443, 444, 448, 455, 456, 457, 459, 460, 462, 464, 466, 467, 469, 470, 472, 473, 475, 480, 484, 491, 493, 494, 496, 497, 507, 509, 511, 513, 516, 520, 527, 533, 534, 535, 539, 547, 556, 557, 560, 567, 573, 590, 591, 596, 597, 601, 602, 603, 630, 631, 632, 634, 635, 640, 663, 665, 669, 670, 671, 672, 678, 682, 687, 688, 690, 693, 696, 697, 699, 704, 709, 716, 718, 723, 724, 726, 737, 742, 743, 745, 751, 752, 757, 768, 776, 777, 779, 780, 781]\n",
        "tx_train_3 = tx_train_2[:, SELECTED]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MK14p1IdRz5",
        "outputId": "f8f7d502-ac8a-4f70-a469-620157612f71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "train_model_3 = lambda y_, x_: make_predictor(reg_logistic_regression_sgd(\n",
        "    y_, x_, 2.1544346900318868e-11, np.zeros(x_.shape[1]), 20, 1000, 0.5,\n",
        ")[0])\n",
        "_ = validation.cross_validation(y_train, tx_train_3, train_model_3, 10, verbose=True)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------ 10-fold cross validation results ------\n",
            "    Accuracy: avg 0.8346, max 0.83808, min 0.8306, stddev 0.0026548\n",
            "    Fbeta score: avg 0.75079, max 0.76022, min 0.73228, stddev 0.0070258\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tle2B4cOyFgT",
        "outputId": "d122e7e6-1f16-4b1e-baff-47fbb9631bc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        }
      },
      "source": [
        "tune_lambda(y_train, tx_train_3, list(map(lambda x: 10 ** (-x / 3), range(9, 40))) + [0])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.001: 0.8330\n",
            "0.00046415888336127773: 0.8336\n",
            "0.00021544346900318845: 0.8342\n",
            "0.0001: 0.8346\n",
            "4.641588833612782e-05: 0.8349\n",
            "2.1544346900318823e-05: 0.8351\n",
            "1e-05: 0.8352\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-1715dd84df4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtune_lambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx_train_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-d4c814108d6f>\u001b[0m in \u001b[0;36mtune_lambda\u001b[0;34m(y, x, grid, seed, history)\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0my_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_init\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         )[0])\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{lambda_}: {res[lambda_]:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/validation.py\u001b[0m in \u001b[0;36mcross_validation\u001b[0;34m(y, x, train_model, number_of_folds, verbose)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_folds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-d4c814108d6f>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(y_, x_)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         train_model = lambda y_, x_: make_predictor(reg_logistic_regression_sgd(\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0my_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_init\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         )[0])\n\u001b[1;32m     11\u001b[0m         \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/implementations.py\u001b[0m in \u001b[0;36mreg_logistic_regression_sgd\u001b[0;34m(y, tx, lambda_, initial_w, n_epochs, batch_size, gamma, history)\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0;31m# calculate gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m             \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogistic_regression_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m             \u001b[0;31m# add L2 regularizer gradient (lambda_ * w^Tw)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m             \u001b[0mg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlambda_\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/implementations.py\u001b[0m in \u001b[0;36mlogistic_regression_grad\u001b[0;34m(y, tx, weights)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlogistic_regression_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m     \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMNiCBVAdPIx"
      },
      "source": [
        "def create_submission(x_train, y_train, x_test, ids, train_model, file_name):\n",
        "  predict = train_model(y_train, x_train)\n",
        "  labels_train = predict(x_train)\n",
        "  train_acc = metrics.accuracy(y_train, labels_train) * 100\n",
        "  full_file_name = f'{file_name}.csv'\n",
        "  if input(f'Train accuracy is {train_acc:.3} %. \\n\\\n",
        "Do you want to continue and create submission `{full_file_name}`? [y/N]').strip().upper() == 'Y':\n",
        "    labels = predict(x_test)\n",
        "    labels = labels * 2 - 1\n",
        "    data_io.create_csv_submission(ids, labels, full_file_name)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMOZXBMYsKyA",
        "outputId": "d61ef4c2-dbbb-4257-fe83-f6cfbc9fadf6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        }
      },
      "source": [
        "SELECTED = [0, 1, 8, 13, 19, 25, 28, 30, 36, 37, 38, 39, 40, 42, 43, 45, 52, 56, 63, 70, 75, 76, 78, 82, 84, 89, 90, 92, 94, 96, 98, 100, 101, 102, 104, 106, 108, 109, 111, 126, 128, 129, 130, 136, 139, 142, 143, 148, 160, 161, 165, 166, 168, 182, 213, 222, 224, 225, 227, 228, 229, 234, 240, 242, 245, 259, 261, 270, 272, 277, 290, 291, 292, 293, 295, 299, 301, 303, 308, 310, 313, 314, 316, 349, 350, 351, 354, 360, 361, 362, 364, 377, 378, 380, 382, 386, 388, 393, 394, 402, 404, 408, 409, 427, 431, 432, 433, 437, 439, 441, 442, 443, 444, 448, 455, 456, 457, 459, 460, 462, 464, 466, 467, 469, 470, 472, 473, 475, 480, 484, 491, 493, 494, 496, 497, 507, 509, 511, 513, 516, 520, 527, 533, 534, 535, 539, 547, 556, 557, 560, 567, 573, 590, 591, 596, 597, 601, 602, 603, 630, 631, 632, 634, 635, 640, 663, 665, 669, 670, 671, 672, 678, 682, 687, 688, 690, 693, 696, 697, 699, 704, 709, 716, 718, 723, 724, 726, 737, 742, 743, 745, 751, 752, 757, 768, 776, 777, 779, 780, 781]\n",
        "\n",
        "tx_train_3 = tx_train_2[:, SELECTED]\n",
        "\n",
        "lambda_, _ = tune_lambda(y_train, tx_train_3, list(map(lambda x: 10 ** (-x / 3), range(9, 40))) + [0])\n",
        "\n",
        "tx_test = transformation_pipeline_median_selected_pairwise(x_test)\n",
        "tx_test = tx_test[:, SELECTED]\n",
        "\n",
        "#lambda_ = 1e-8\n",
        "train_model = lambda y_, x_: make_predictor(reg_logistic_regression_sgd(\n",
        "    y_, x_, lambda_, np.zeros(x_.shape[1]), 20, 1000, 0.5,\n",
        ")[0])\n",
        "\n",
        "create_submission(\n",
        "    tx_train_3, y_train, \n",
        "    tx_test, \n",
        "    ids_test, train_model, '836accSub')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.001: 0.8330\n",
            "0.00046415888336127773: 0.8336\n",
            "0.00021544346900318845: 0.8342\n",
            "0.0001: 0.8346\n",
            "4.641588833612782e-05: 0.8349\n",
            "2.1544346900318823e-05: 0.8351\n",
            "1e-05: 0.8352\n",
            "4.641588833612782e-06: 0.8354\n",
            "2.1544346900318822e-06: 0.8355\n",
            "1e-06: 0.8357\n",
            "4.641588833612782e-07: 0.8357\n",
            "2.1544346900318822e-07: 0.8357\n",
            "1e-07: 0.8357\n",
            "4.641588833612782e-08: 0.8358\n",
            "2.1544346900318822e-08: 0.8358\n",
            "1e-08: 0.8358\n",
            "4.641588833612773e-09: 0.8358\n",
            "2.1544346900318866e-09: 0.8358\n",
            "1e-09: 0.8358\n",
            "4.6415888336127727e-10: 0.8358\n",
            "2.1544346900318867e-10: 0.8358\n",
            "1e-10: 0.8357\n",
            "4.641588833612773e-11: 0.8358\n",
            "2.1544346900318868e-11: 0.8358\n",
            "1e-11: 0.8358\n",
            "4.6415888336127725e-12: 0.8358\n",
            "2.1544346900318868e-12: 0.8358\n",
            "1e-12: 0.8358\n",
            "4.641588833612772e-13: 0.8358\n",
            "2.1544346900318868e-13: 0.8358\n",
            "1e-13: 0.8358\n",
            "0: 0.8358\n",
            "Train accuracy is 83.5 %. \n",
            "Do you want to continue and create submission `836accSub.csv`? [y/N]y\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqqZJn9Zlz0N"
      },
      "source": [
        "NEW IDEA:\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylyFFYHIlOXV"
      },
      "source": [
        "def get_top_abs_k_indexes(arr, k):\n",
        "  indexes = np.abs(arr).argsort()[-k:]\n",
        "  indexes.sort()\n",
        "  return indexes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ag7LkuVl3UR",
        "outputId": "612e78b4-a4a8-4af8-83b5-e5d9917f4009",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import sys\n",
        "import numpy\n",
        "import time\n",
        "numpy.set_printoptions(threshold=sys.maxsize)\n",
        "\n",
        "for i in range(20):\n",
        "  lasso_lambda_ = 10 ** (-i / 3)\n",
        "  time.sleep(1) # To clear RAM if needed\n",
        "  lasso_w, _ = lasso_logistic_regression_sgd(y_train, tx_train_2, .1, np.zeros(tx_train_2.shape[1]), 5, 1000, .5)\n",
        "  for k_subset in [20, 30, 40, 50, 80, 100, 150, 200]:\n",
        "    time.sleep(1)\n",
        "    top_k_w = get_top_abs_k_indexes(lasso_w, k_subset)\n",
        "\n",
        "    features_subset = tx_train_2[:, top_k_w]\n",
        "\n",
        "    print(f'Lambda {lasso_lambda_}, top_k {k_subset}')\n",
        "    accs, _ = validation.cross_validation(y_train, features_subset, train_model, 10, verbose=True)\n",
        "    mean_acc = np.mean(accs)\n",
        "    with open(f'{DATA_FILE_PREFIX}lasso_data_no_abs.txt', 'a') as f:\n",
        "      list_top_k = list(top_k_w)\n",
        "      print(f'lambda {lasso_lambda_}, k {k_subset}: accuracy {mean_acc}, top features {list_top_k}', file=f)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/implementations.py:197: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1 + np.exp(-x))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Lambda 1.0, top_k 20\n",
            "------ 10-fold cross validation results ------\n",
            "    Accuracy: avg 0.62575, max 0.6362, min 0.61204, stddev 0.0076967\n",
            "    Fbeta score: avg 0.42027, max 0.54746, min 0.23237, stddev 0.09181\n",
            "Lambda 1.0, top_k 30\n",
            "------ 10-fold cross validation results ------\n",
            "    Accuracy: avg 0.69394, max 0.71152, min 0.68024, stddev 0.011197\n",
            "    Fbeta score: avg 0.49158, max 0.61369, min 0.27446, stddev 0.12087\n",
            "Lambda 1.0, top_k 40\n",
            "------ 10-fold cross validation results ------\n",
            "    Accuracy: avg 0.71472, max 0.73928, min 0.68548, stddev 0.014337\n",
            "    Fbeta score: avg 0.56265, max 0.64976, min 0.32803, stddev 0.12231\n",
            "Lambda 1.0, top_k 50\n",
            "------ 10-fold cross validation results ------\n",
            "    Accuracy: avg 0.70006, max 0.72932, min 0.66408, stddev 0.019329\n",
            "    Fbeta score: avg 0.32408, max 0.62942, min 0.16413, stddev 0.13141\n",
            "Lambda 1.0, top_k 80\n",
            "------ 10-fold cross validation results ------\n",
            "    Accuracy: avg 0.70761, max 0.72736, min 0.68304, stddev 0.011324\n",
            "    Fbeta score: avg 0.33474, max 0.65267, min 0.22761, stddev 0.11298\n",
            "Lambda 1.0, top_k 100\n",
            "------ 10-fold cross validation results ------\n",
            "    Accuracy: avg 0.70432, max 0.7244, min 0.66908, stddev 0.014882\n",
            "    Fbeta score: avg 0.32157, max 0.64147, min 0.17507, stddev 0.11654\n",
            "Lambda 1.0, top_k 150\n",
            "------ 10-fold cross validation results ------\n",
            "    Accuracy: avg 0.71637, max 0.72856, min 0.70492, stddev 0.0083772\n",
            "    Fbeta score: avg 0.31668, max 0.37456, min 0.2609, stddev 0.041974\n",
            "Lambda 1.0, top_k 200\n",
            "------ 10-fold cross validation results ------\n",
            "    Accuracy: avg 0.72982, max 0.75988, min 0.70492, stddev 0.018292\n",
            "    Fbeta score: avg 0.37059, max 0.50004, min 0.25658, stddev 0.081146\n",
            "Lambda 0.4641588833612779, top_k 20\n",
            "------ 10-fold cross validation results ------\n",
            "    Accuracy: avg 0.67918, max 0.69756, min 0.62552, stddev 0.019739\n",
            "    Fbeta score: avg 0.45926, max 0.48334, min 0.43088, stddev 0.017621\n",
            "Lambda 0.4641588833612779, top_k 30\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/implementations.py:207: RuntimeWarning: overflow encountered in exp\n",
            "  loss = np.log1p(np.exp(-t * (tx @ weights)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------ 10-fold cross validation results ------\n",
            "    Accuracy: avg 0.72706, max 0.73628, min 0.722, stddev 0.0043651\n",
            "    Fbeta score: avg 0.52364, max 0.55306, min 0.48859, stddev 0.023522\n",
            "Lambda 0.4641588833612779, top_k 40\n",
            "------ 10-fold cross validation results ------\n",
            "    Accuracy: avg 0.75232, max 0.76164, min 0.74336, stddev 0.0060446\n",
            "    Fbeta score: avg 0.60091, max 0.63941, min 0.53468, stddev 0.031121\n",
            "Lambda 0.4641588833612779, top_k 50\n",
            "------ 10-fold cross validation results ------\n",
            "    Accuracy: avg 0.74219, max 0.757, min 0.69876, stddev 0.017514\n",
            "    Fbeta score: avg 0.5642, max 0.64802, min 0.39913, stddev 0.069109\n",
            "Lambda 0.4641588833612779, top_k 80\n",
            "------ 10-fold cross validation results ------\n",
            "    Accuracy: avg 0.75627, max 0.78444, min 0.73616, stddev 0.017005\n",
            "    Fbeta score: avg 0.62857, max 0.67248, min 0.5416, stddev 0.039952\n",
            "Lambda 0.4641588833612779, top_k 100\n",
            "------ 10-fold cross validation results ------\n",
            "    Accuracy: avg 0.70895, max 0.74632, min 0.68628, stddev 0.020589\n",
            "    Fbeta score: avg 0.48874, max 0.68309, min 0.18664, stddev 0.19323\n",
            "Lambda 0.4641588833612779, top_k 150\n",
            "------ 10-fold cross validation results ------\n",
            "    Accuracy: avg 0.73944, max 0.76988, min 0.7082, stddev 0.01767\n",
            "    Fbeta score: avg 0.52179, max 0.69049, min 0.37717, stddev 0.11678\n",
            "Lambda 0.4641588833612779, top_k 200\n",
            "------ 10-fold cross validation results ------\n",
            "    Accuracy: avg 0.7225, max 0.74636, min 0.67676, stddev 0.018294\n",
            "    Fbeta score: avg 0.52855, max 0.69972, min 0.3536, stddev 0.15671\n",
            "Lambda 0.2154434690031884, top_k 20\n",
            "------ 10-fold cross validation results ------\n",
            "    Accuracy: avg 0.71416, max 0.73404, min 0.69528, stddev 0.012458\n",
            "    Fbeta score: avg 0.32761, max 0.43734, min 0.22628, stddev 0.065511\n",
            "Lambda 0.2154434690031884, top_k 30\n",
            "------ 10-fold cross validation results ------\n",
            "    Accuracy: avg 0.65721, max 0.71644, min 0.60816, stddev 0.036187\n",
            "    Fbeta score: avg 0.52633, max 0.63489, min 0.19681, stddev 0.15845\n",
            "Lambda 0.2154434690031884, top_k 40\n",
            "------ 10-fold cross validation results ------\n",
            "    Accuracy: avg 0.65429, max 0.70816, min 0.60568, stddev 0.035813\n",
            "    Fbeta score: avg 0.46615, max 0.63546, min 0.17223, stddev 0.19972\n",
            "Lambda 0.2154434690031884, top_k 50\n",
            "------ 10-fold cross validation results ------\n",
            "    Accuracy: avg 0.67068, max 0.71684, min 0.59568, stddev 0.040953\n",
            "    Fbeta score: avg 0.40096, max 0.63342, min 0.14206, stddev 0.19053\n",
            "Lambda 0.2154434690031884, top_k 80\n",
            "------ 10-fold cross validation results ------\n",
            "    Accuracy: avg 0.67757, max 0.71116, min 0.61432, stddev 0.032123\n",
            "    Fbeta score: avg 0.43092, max 0.65529, min 0.13748, stddev 0.21208\n",
            "Lambda 0.2154434690031884, top_k 100\n",
            "------ 10-fold cross validation results ------\n",
            "    Accuracy: avg 0.653, max 0.69776, min 0.59024, stddev 0.032533\n",
            "    Fbeta score: avg 0.54346, max 0.65412, min 0.16432, stddev 0.1831\n",
            "Lambda 0.2154434690031884, top_k 150\n",
            "------ 10-fold cross validation results ------\n",
            "    Accuracy: avg 0.68642, max 0.7366, min 0.63208, stddev 0.031533\n",
            "    Fbeta score: avg 0.42577, max 0.65161, min 0.16242, stddev 0.18541\n",
            "Lambda 0.2154434690031884, top_k 200\n",
            "------ 10-fold cross validation results ------\n",
            "    Accuracy: avg 0.70332, max 0.73132, min 0.66728, stddev 0.01975\n",
            "    Fbeta score: avg 0.41464, max 0.66291, min 0.25443, stddev 0.16098\n",
            "Lambda 0.1, top_k 20\n",
            "------ 10-fold cross validation results ------\n",
            "    Accuracy: avg 0.72591, max 0.73216, min 0.7106, stddev 0.0059996\n",
            "    Fbeta score: avg 0.54362, max 0.59574, min 0.48985, stddev 0.031422\n",
            "Lambda 0.1, top_k 30\n",
            "------ 10-fold cross validation results ------\n",
            "    Accuracy: avg 0.74604, max 0.75572, min 0.73804, stddev 0.0054074\n",
            "    Fbeta score: avg 0.5777, max 0.66289, min 0.45848, stddev 0.060646\n",
            "Lambda 0.1, top_k 40\n",
            "------ 10-fold cross validation results ------\n",
            "    Accuracy: avg 0.70821, max 0.72256, min 0.68924, stddev 0.0095739\n",
            "    Fbeta score: avg 0.27976, max 0.35129, min 0.1835, stddev 0.048686\n",
            "Lambda 0.1, top_k 50\n",
            "------ 10-fold cross validation results ------\n",
            "    Accuracy: avg 0.70242, max 0.72668, min 0.67708, stddev 0.013585\n",
            "    Fbeta score: avg 0.25088, max 0.38, min 0.11626, stddev 0.071534\n",
            "Lambda 0.1, top_k 80\n",
            "------ 10-fold cross validation results ------\n",
            "    Accuracy: avg 0.66859, max 0.70944, min 0.62024, stddev 0.032734\n",
            "    Fbeta score: avg 0.47818, max 0.65212, min 0.18103, stddev 0.19608\n",
            "Lambda 0.1, top_k 100\n",
            "------ 10-fold cross validation results ------\n",
            "    Accuracy: avg 0.68502, max 0.73032, min 0.62316, stddev 0.038033\n",
            "    Fbeta score: avg 0.43366, max 0.65036, min 0.23188, stddev 0.1692\n",
            "Lambda 0.1, top_k 150\n",
            "------ 10-fold cross validation results ------\n",
            "    Accuracy: avg 0.68512, max 0.71824, min 0.63748, stddev 0.027876\n",
            "    Fbeta score: avg 0.50664, max 0.66512, min 0.21014, stddev 0.17883\n",
            "Lambda 0.1, top_k 200\n",
            "------ 10-fold cross validation results ------\n",
            "    Accuracy: avg 0.68658, max 0.7452, min 0.6188, stddev 0.039748\n",
            "    Fbeta score: avg 0.4896, max 0.66612, min 0.28374, stddev 0.16153\n",
            "Lambda 0.046415888336127795, top_k 20\n",
            "------ 10-fold cross validation results ------\n",
            "    Accuracy: avg 0.72848, max 0.73464, min 0.72084, stddev 0.0037694\n",
            "    Fbeta score: avg 0.52793, max 0.55094, min 0.49606, stddev 0.014906\n",
            "Lambda 0.046415888336127795, top_k 30\n",
            "------ 10-fold cross validation results ------\n",
            "    Accuracy: avg 0.74172, max 0.75196, min 0.73548, stddev 0.0048301\n",
            "    Fbeta score: avg 0.53829, max 0.59235, min 0.49197, stddev 0.030318\n",
            "Lambda 0.046415888336127795, top_k 40\n",
            "------ 10-fold cross validation results ------\n",
            "    Accuracy: avg 0.74176, max 0.75768, min 0.7196, stddev 0.010606\n",
            "    Fbeta score: avg 0.58401, max 0.63712, min 0.48366, stddev 0.051524\n",
            "Lambda 0.046415888336127795, top_k 50\n",
            "------ 10-fold cross validation results ------\n",
            "    Accuracy: avg 0.73465, max 0.75572, min 0.69168, stddev 0.01954\n",
            "    Fbeta score: avg 0.5674, max 0.64179, min 0.3146, stddev 0.11639\n",
            "Lambda 0.046415888336127795, top_k 80\n",
            "------ 10-fold cross validation results ------\n",
            "    Accuracy: avg 0.70553, max 0.72464, min 0.69112, stddev 0.0099089\n",
            "    Fbeta score: avg 0.26962, max 0.37464, min 0.19546, stddev 0.052871\n",
            "Lambda 0.046415888336127795, top_k 100\n",
            "------ 10-fold cross validation results ------\n",
            "    Accuracy: avg 0.7038, max 0.72732, min 0.68304, stddev 0.013904\n",
            "    Fbeta score: avg 0.25859, max 0.38033, min 0.14942, stddev 0.072152\n",
            "Lambda 0.046415888336127795, top_k 150\n",
            "------ 10-fold cross validation results ------\n",
            "    Accuracy: avg 0.70461, max 0.75236, min 0.64932, stddev 0.029356\n",
            "    Fbeta score: avg 0.38169, max 0.65176, min 0.19704, stddev 0.15884\n",
            "Lambda 0.046415888336127795, top_k 200\n",
            "------ 10-fold cross validation results ------\n",
            "    Accuracy: avg 0.67089, max 0.71476, min 0.61968, stddev 0.02792\n",
            "    Fbeta score: avg 0.57531, max 0.66375, min 0.25419, stddev 0.14781\n",
            "Lambda 0.021544346900318832, top_k 20\n",
            "------ 10-fold cross validation results ------\n",
            "    Accuracy: avg 0.69572, max 0.70008, min 0.68672, stddev 0.0036439\n",
            "    Fbeta score: avg 0.47885, max 0.4937, min 0.4643, stddev 0.0085996\n",
            "Lambda 0.021544346900318832, top_k 30\n",
            "------ 10-fold cross validation results ------\n",
            "    Accuracy: avg 0.68956, max 0.69492, min 0.67868, stddev 0.0049551\n",
            "    Fbeta score: avg 0.46358, max 0.47433, min 0.44001, stddev 0.0098282\n",
            "Lambda 0.021544346900318832, top_k 40\n",
            "------ 10-fold cross validation results ------\n",
            "    Accuracy: avg 0.72756, max 0.73416, min 0.71988, stddev 0.0040567\n",
            "    Fbeta score: avg 0.55223, max 0.57256, min 0.53791, stddev 0.010778\n",
            "Lambda 0.021544346900318832, top_k 50\n",
            "------ 10-fold cross validation results ------\n",
            "    Accuracy: avg 0.72818, max 0.73552, min 0.71088, stddev 0.0083755\n",
            "    Fbeta score: avg 0.56052, max 0.58289, min 0.5274, stddev 0.017169\n",
            "Lambda 0.021544346900318832, top_k 80\n",
            "------ 10-fold cross validation results ------\n",
            "    Accuracy: avg 0.73113, max 0.74616, min 0.70784, stddev 0.01355\n",
            "    Fbeta score: avg 0.57922, max 0.61227, min 0.53359, stddev 0.025601\n",
            "Lambda 0.021544346900318832, top_k 100\n",
            "------ 10-fold cross validation results ------\n",
            "    Accuracy: avg 0.769, max 0.79072, min 0.75652, stddev 0.010033\n",
            "    Fbeta score: avg 0.65409, max 0.68588, min 0.6154, stddev 0.021552\n",
            "Lambda 0.021544346900318832, top_k 150\n",
            "------ 10-fold cross validation results ------\n",
            "    Accuracy: avg 0.74431, max 0.78004, min 0.7284, stddev 0.013714\n",
            "    Fbeta score: avg 0.62215, max 0.68683, min 0.5756, stddev 0.035745\n",
            "Lambda 0.021544346900318832, top_k 200\n",
            "------ 10-fold cross validation results ------\n",
            "    Accuracy: avg 0.74386, max 0.78208, min 0.7168, stddev 0.020671\n",
            "    Fbeta score: avg 0.61523, max 0.67342, min 0.56985, stddev 0.040684\n",
            "Lambda 0.01, top_k 20\n",
            "------ 10-fold cross validation results ------\n",
            "    Accuracy: avg 0.75168, max 0.7572, min 0.7474, stddev 0.0031295\n",
            "    Fbeta score: avg 0.59085, max 0.61587, min 0.57447, stddev 0.012981\n",
            "Lambda 0.01, top_k 30\n",
            "------ 10-fold cross validation results ------\n",
            "    Accuracy: avg 0.75154, max 0.75908, min 0.7438, stddev 0.0049636\n",
            "    Fbeta score: avg 0.5666, max 0.63435, min 0.49598, stddev 0.042028\n",
            "Lambda 0.01, top_k 40\n",
            "------ 10-fold cross validation results ------\n",
            "    Accuracy: avg 0.73162, max 0.7572, min 0.71136, stddev 0.017822\n",
            "    Fbeta score: avg 0.5415, max 0.65201, min 0.32472, stddev 0.13364\n",
            "Lambda 0.01, top_k 50\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-7ea61be7cae2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Lambda {lasso_lambda_}, top_k {k_subset}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0maccs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_subset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mmean_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{DATA_FILE_PREFIX}lasso_data_no_abs.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/validation.py\u001b[0m in \u001b[0;36mcross_validation\u001b[0;34m(y, x, train_model, number_of_folds, verbose)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_folds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-f64ca69744c3>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(y, x)\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mlambda_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   return make_predictor(reg_logistic_regression_sgd(\n\u001b[0;32m----> 5\u001b[0;31m       y, x, lambda_, w_init, 5, 1000, 0.5)[0])\n\u001b[0m",
            "\u001b[0;32m/content/implementations.py\u001b[0m in \u001b[0;36mreg_logistic_regression_sgd\u001b[0;34m(y, tx, lambda_, initial_w, n_epochs, batch_size, gamma, history)\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0;31m# calculate gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m             \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogistic_regression_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m             \u001b[0;31m# add L2 regularizer gradient (lambda_ * w^Tw)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m             \u001b[0mg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlambda_\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6p0Dp3yuSVZ",
        "outputId": "336093a3-28c8-479e-c269-99e36573e8be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import sys\n",
        "import numpy\n",
        "import time\n",
        "numpy.set_printoptions(threshold=sys.maxsize)\n",
        "\n",
        "for i in range(-3, 30):\n",
        "  lasso_lambda_ = 10 ** (-i / 5)\n",
        "  time.sleep(1) # To clear RAM if needed\n",
        "  lasso_w, _ = lasso_logistic_regression_sgd(y_train, tx_train_2, lasso_lambda_, np.zeros(tx_train_2.shape[1]), 5, 500, .3)\n",
        "  for k_subset in [20, 30, 40, 50, 80, 100, 150, 200, 250, 300, 350, 400]:\n",
        "    time.sleep(1)\n",
        "    top_k_w = get_top_abs_k_indexes(lasso_w, k_subset)\n",
        "\n",
        "    features_subset = tx_train_2[:, top_k_w]\n",
        "\n",
        "    print(f'Lambda {lasso_lambda_}, top_k {k_subset}')\n",
        "    best_ridge_lambda, mean_acc = tune_lambda(\n",
        "        y_train, features_subset, [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9], history=False)\n",
        "    print(f'Ridge lambda {best_ridge_lambda}, accuracy {mean_acc}')\n",
        "    with open(f'{DATA_FILE_PREFIX}lasso_data_no_abs_granular_no_plus.txt', 'a') as f:\n",
        "      list_top_k = list(top_k_w)\n",
        "      print(f'lambda {lasso_lambda_}, k {k_subset}: accuracy {mean_acc}, best ridge lambda {best_ridge_lambda}, top features \\n{list_top_k}\\n\\n\\n', file=f)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/implementations.py:197: RuntimeWarning: overflow encountered in exp\n",
            "  exp_x = np.exp(x)\n",
            "/content/implementations.py:200: RuntimeWarning: invalid value encountered in true_divide\n",
            "  exp_x / (1 + exp_x))\n",
            "/content/implementations.py:199: RuntimeWarning: overflow encountered in exp\n",
            "  1 / (1 + np.exp(-x)),\n",
            "/content/implementations.py:211: RuntimeWarning: overflow encountered in exp\n",
            "  loss = np.log1p(np.exp(-t * (tx @ weights)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Lambda 3.9810717055349722, top_k 20\n",
            "Ridge lambda 1e-06, accuracy 0.724696\n",
            "Lambda 3.9810717055349722, top_k 30\n",
            "Ridge lambda 1e-07, accuracy 0.73758\n",
            "Lambda 3.9810717055349722, top_k 40\n",
            "Ridge lambda 1e-09, accuracy 0.745796\n",
            "Lambda 3.9810717055349722, top_k 50\n",
            "Ridge lambda 1e-09, accuracy 0.770696\n",
            "Lambda 3.9810717055349722, top_k 80\n",
            "Ridge lambda 1e-08, accuracy 0.81648\n",
            "Lambda 3.9810717055349722, top_k 100\n",
            "Ridge lambda 1e-06, accuracy 0.820852\n",
            "Lambda 3.9810717055349722, top_k 150\n",
            "Ridge lambda 1e-09, accuracy 0.8247959999999999\n",
            "Lambda 3.9810717055349722, top_k 200\n",
            "Ridge lambda 1e-09, accuracy 0.82854\n",
            "Lambda 3.9810717055349722, top_k 250\n",
            "Ridge lambda 1e-09, accuracy 0.830964\n",
            "Lambda 3.9810717055349722, top_k 300\n",
            "Ridge lambda 1e-09, accuracy 0.8314280000000001\n",
            "Lambda 3.9810717055349722, top_k 350\n",
            "Ridge lambda 1e-09, accuracy 0.831952\n",
            "Lambda 3.9810717055349722, top_k 400\n",
            "Ridge lambda 0.0001, accuracy 0.8309159999999999\n",
            "Lambda 2.51188643150958, top_k 20\n",
            "Ridge lambda 1e-06, accuracy 0.71654\n",
            "Lambda 2.51188643150958, top_k 30\n",
            "Ridge lambda 1e-08, accuracy 0.76552\n",
            "Lambda 2.51188643150958, top_k 40\n",
            "Ridge lambda 1e-09, accuracy 0.775188\n",
            "Lambda 2.51188643150958, top_k 50\n",
            "Ridge lambda 1e-08, accuracy 0.790888\n",
            "Lambda 2.51188643150958, top_k 80\n",
            "Ridge lambda 1e-09, accuracy 0.8098879999999999\n",
            "Lambda 2.51188643150958, top_k 100\n",
            "Ridge lambda 1e-09, accuracy 0.815796\n",
            "Lambda 2.51188643150958, top_k 150\n",
            "Ridge lambda 1e-09, accuracy 0.8243839999999999\n",
            "Lambda 2.51188643150958, top_k 200\n",
            "Ridge lambda 1e-09, accuracy 0.827392\n",
            "Lambda 2.51188643150958, top_k 250\n",
            "Ridge lambda 1e-08, accuracy 0.8291920000000002\n",
            "Lambda 2.51188643150958, top_k 300\n",
            "Ridge lambda 1e-09, accuracy 0.8289519999999999\n",
            "Lambda 2.51188643150958, top_k 350\n",
            "Ridge lambda 1e-09, accuracy 0.8288\n",
            "Lambda 2.51188643150958, top_k 400\n",
            "Ridge lambda 0.001, accuracy 0.82804\n",
            "Lambda 1.5848931924611136, top_k 20\n",
            "Ridge lambda 1e-06, accuracy 0.711496\n",
            "Lambda 1.5848931924611136, top_k 30\n",
            "Ridge lambda 1e-07, accuracy 0.72862\n",
            "Lambda 1.5848931924611136, top_k 40\n",
            "Ridge lambda 1e-09, accuracy 0.7686\n",
            "Lambda 1.5848931924611136, top_k 50\n",
            "Ridge lambda 1e-08, accuracy 0.776296\n",
            "Lambda 1.5848931924611136, top_k 80\n",
            "Ridge lambda 1e-09, accuracy 0.81182\n",
            "Lambda 1.5848931924611136, top_k 100\n",
            "Ridge lambda 1e-09, accuracy 0.819384\n",
            "Lambda 1.5848931924611136, top_k 150\n",
            "Ridge lambda 1e-09, accuracy 0.825128\n",
            "Lambda 1.5848931924611136, top_k 200\n",
            "Ridge lambda 1e-09, accuracy 0.828496\n",
            "Lambda 1.5848931924611136, top_k 250\n",
            "Ridge lambda 1e-09, accuracy 0.830128\n",
            "Lambda 1.5848931924611136, top_k 300\n",
            "Ridge lambda 1e-07, accuracy 0.830908\n",
            "Lambda 1.5848931924611136, top_k 350\n",
            "Ridge lambda 1e-09, accuracy 0.8324719999999999\n",
            "Lambda 1.5848931924611136, top_k 400\n",
            "Ridge lambda 1e-09, accuracy 0.83116\n",
            "Lambda 1.0, top_k 20\n",
            "Ridge lambda 1e-06, accuracy 0.6764\n",
            "Lambda 1.0, top_k 30\n",
            "Ridge lambda 1e-08, accuracy 0.728476\n",
            "Lambda 1.0, top_k 40\n",
            "Ridge lambda 1e-08, accuracy 0.7424320000000001\n",
            "Lambda 1.0, top_k 50\n",
            "Ridge lambda 1e-06, accuracy 0.7619360000000001\n",
            "Lambda 1.0, top_k 80\n",
            "Ridge lambda 1e-09, accuracy 0.780752\n",
            "Lambda 1.0, top_k 100\n",
            "Ridge lambda 1e-09, accuracy 0.7951079999999999\n",
            "Lambda 1.0, top_k 150\n",
            "Ridge lambda 1e-09, accuracy 0.8182\n",
            "Lambda 1.0, top_k 200\n",
            "Ridge lambda 1e-09, accuracy 0.8242839999999999\n",
            "Lambda 1.0, top_k 250\n",
            "Ridge lambda 1e-07, accuracy 0.829204\n",
            "Lambda 1.0, top_k 300\n",
            "Ridge lambda 1e-07, accuracy 0.8309559999999999\n",
            "Lambda 1.0, top_k 350\n",
            "Ridge lambda 1e-09, accuracy 0.833596\n",
            "Lambda 1.0, top_k 400\n",
            "Ridge lambda 1e-09, accuracy 0.833484\n",
            "Lambda 0.6309573444801932, top_k 20\n",
            "Ridge lambda 1e-08, accuracy 0.732268\n",
            "Lambda 0.6309573444801932, top_k 30\n",
            "Ridge lambda 1e-09, accuracy 0.7351599999999999\n",
            "Lambda 0.6309573444801932, top_k 40\n",
            "Ridge lambda 1e-09, accuracy 0.752768\n",
            "Lambda 0.6309573444801932, top_k 50\n",
            "Ridge lambda 1e-09, accuracy 0.77052\n",
            "Lambda 0.6309573444801932, top_k 80\n",
            "Ridge lambda 1e-09, accuracy 0.799664\n",
            "Lambda 0.6309573444801932, top_k 100\n",
            "Ridge lambda 1e-09, accuracy 0.8090080000000001\n",
            "Lambda 0.6309573444801932, top_k 150\n",
            "Ridge lambda 1e-09, accuracy 0.820112\n",
            "Lambda 0.6309573444801932, top_k 200\n",
            "Ridge lambda 1e-09, accuracy 0.821612\n",
            "Lambda 0.6309573444801932, top_k 250\n",
            "Ridge lambda 1e-09, accuracy 0.8223559999999999\n",
            "Lambda 0.6309573444801932, top_k 300\n",
            "Ridge lambda 0.0001, accuracy 0.8286719999999999\n",
            "Lambda 0.6309573444801932, top_k 350\n",
            "Ridge lambda 1e-09, accuracy 0.827704\n",
            "Lambda 0.6309573444801932, top_k 400\n",
            "Ridge lambda 0.0001, accuracy 0.798324\n",
            "Lambda 0.3981071705534972, top_k 20\n",
            "Ridge lambda 1e-09, accuracy 0.723916\n",
            "Lambda 0.3981071705534972, top_k 30\n",
            "Ridge lambda 1e-09, accuracy 0.741532\n",
            "Lambda 0.3981071705534972, top_k 40\n",
            "Ridge lambda 1e-09, accuracy 0.7463\n",
            "Lambda 0.3981071705534972, top_k 50\n",
            "Ridge lambda 1e-09, accuracy 0.751044\n",
            "Lambda 0.3981071705534972, top_k 80\n",
            "Ridge lambda 1e-09, accuracy 0.7761399999999999\n",
            "Lambda 0.3981071705534972, top_k 100\n",
            "Ridge lambda 1e-09, accuracy 0.795556\n",
            "Lambda 0.3981071705534972, top_k 150\n",
            "Ridge lambda 1e-09, accuracy 0.820692\n",
            "Lambda 0.3981071705534972, top_k 200\n",
            "Ridge lambda 1e-09, accuracy 0.8253119999999999\n",
            "Lambda 0.3981071705534972, top_k 250\n",
            "Ridge lambda 1e-09, accuracy 0.829852\n",
            "Lambda 0.3981071705534972, top_k 300\n",
            "Ridge lambda 1e-09, accuracy 0.829108\n",
            "Lambda 0.3981071705534972, top_k 350\n",
            "Ridge lambda 1e-09, accuracy 0.826876\n",
            "Lambda 0.3981071705534972, top_k 400\n",
            "Ridge lambda 0.0001, accuracy 0.823056\n",
            "Lambda 0.251188643150958, top_k 20\n",
            "Ridge lambda 1e-07, accuracy 0.6934359999999999\n",
            "Lambda 0.251188643150958, top_k 30\n",
            "Ridge lambda 1e-09, accuracy 0.716876\n",
            "Lambda 0.251188643150958, top_k 40\n",
            "Ridge lambda 1e-09, accuracy 0.72868\n",
            "Lambda 0.251188643150958, top_k 50\n",
            "Ridge lambda 1e-09, accuracy 0.7406720000000001\n",
            "Lambda 0.251188643150958, top_k 80\n",
            "Ridge lambda 1e-09, accuracy 0.7792999999999999\n",
            "Lambda 0.251188643150958, top_k 100\n",
            "Ridge lambda 1e-07, accuracy 0.8021239999999998\n",
            "Lambda 0.251188643150958, top_k 150\n",
            "Ridge lambda 1e-09, accuracy 0.8148199999999999\n",
            "Lambda 0.251188643150958, top_k 200\n",
            "Ridge lambda 1e-06, accuracy 0.8201839999999999\n",
            "Lambda 0.251188643150958, top_k 250\n",
            "Ridge lambda 1e-09, accuracy 0.823972\n",
            "Lambda 0.251188643150958, top_k 300\n",
            "Ridge lambda 1e-09, accuracy 0.82882\n",
            "Lambda 0.251188643150958, top_k 350\n",
            "Ridge lambda 1e-09, accuracy 0.8303039999999999\n",
            "Lambda 0.251188643150958, top_k 400\n",
            "Ridge lambda 1e-09, accuracy 0.8262799999999999\n",
            "Lambda 0.15848931924611134, top_k 20\n",
            "Ridge lambda 0.0001, accuracy 0.7076439999999999\n",
            "Lambda 0.15848931924611134, top_k 30\n",
            "Ridge lambda 1e-08, accuracy 0.722236\n",
            "Lambda 0.15848931924611134, top_k 40\n",
            "Ridge lambda 1e-07, accuracy 0.725936\n",
            "Lambda 0.15848931924611134, top_k 50\n",
            "Ridge lambda 1e-09, accuracy 0.737176\n",
            "Lambda 0.15848931924611134, top_k 80\n",
            "Ridge lambda 1e-07, accuracy 0.7645919999999999\n",
            "Lambda 0.15848931924611134, top_k 100\n",
            "Ridge lambda 1e-08, accuracy 0.78628\n",
            "Lambda 0.15848931924611134, top_k 150\n",
            "Ridge lambda 1e-09, accuracy 0.8173119999999999\n",
            "Lambda 0.15848931924611134, top_k 200\n",
            "Ridge lambda 1e-09, accuracy 0.823212\n",
            "Lambda 0.15848931924611134, top_k 250\n",
            "Ridge lambda 1e-09, accuracy 0.827396\n",
            "Lambda 0.15848931924611134, top_k 300\n",
            "Ridge lambda 1e-09, accuracy 0.829528\n",
            "Lambda 0.15848931924611134, top_k 350\n",
            "Ridge lambda 1e-09, accuracy 0.829252\n",
            "Lambda 0.15848931924611134, top_k 400\n",
            "Ridge lambda 1e-09, accuracy 0.8308879999999998\n",
            "Lambda 0.1, top_k 20\n",
            "Ridge lambda 1e-09, accuracy 0.693648\n",
            "Lambda 0.1, top_k 30\n",
            "Ridge lambda 1e-08, accuracy 0.715604\n",
            "Lambda 0.1, top_k 40\n",
            "Ridge lambda 1e-09, accuracy 0.728348\n",
            "Lambda 0.1, top_k 50\n",
            "Ridge lambda 1e-09, accuracy 0.7443960000000001\n",
            "Lambda 0.1, top_k 80\n",
            "Ridge lambda 1e-09, accuracy 0.754232\n",
            "Lambda 0.1, top_k 100\n",
            "Ridge lambda 1e-08, accuracy 0.708016\n",
            "Lambda 0.1, top_k 150\n",
            "Ridge lambda 1e-09, accuracy 0.729828\n",
            "Lambda 0.1, top_k 200\n",
            "Ridge lambda 1e-09, accuracy 0.7602599999999999\n",
            "Lambda 0.1, top_k 250\n",
            "Ridge lambda 1e-09, accuracy 0.772536\n",
            "Lambda 0.1, top_k 300\n",
            "Ridge lambda 0.0001, accuracy 0.7716799999999999\n",
            "Lambda 0.1, top_k 350\n",
            "Ridge lambda 1e-08, accuracy 0.781652\n",
            "Lambda 0.1, top_k 400\n",
            "Ridge lambda 1e-09, accuracy 0.7830959999999999\n",
            "Lambda 0.06309573444801933, top_k 20\n",
            "Ridge lambda 1e-08, accuracy 0.699724\n",
            "Lambda 0.06309573444801933, top_k 30\n",
            "Ridge lambda 1e-09, accuracy 0.7255320000000001\n",
            "Lambda 0.06309573444801933, top_k 40\n",
            "Ridge lambda 1e-09, accuracy 0.7445039999999999\n",
            "Lambda 0.06309573444801933, top_k 50\n",
            "Ridge lambda 1e-06, accuracy 0.75484\n",
            "Lambda 0.06309573444801933, top_k 80\n",
            "Ridge lambda 1e-08, accuracy 0.771876\n",
            "Lambda 0.06309573444801933, top_k 100\n",
            "Ridge lambda 1e-09, accuracy 0.7877959999999999\n",
            "Lambda 0.06309573444801933, top_k 150\n",
            "Ridge lambda 1e-09, accuracy 0.803964\n",
            "Lambda 0.06309573444801933, top_k 200\n",
            "Ridge lambda 1e-09, accuracy 0.817548\n",
            "Lambda 0.06309573444801933, top_k 250\n",
            "Ridge lambda 1e-09, accuracy 0.82104\n",
            "Lambda 0.06309573444801933, top_k 300\n",
            "Ridge lambda 1e-09, accuracy 0.8239879999999999\n",
            "Lambda 0.06309573444801933, top_k 350\n",
            "Ridge lambda 1e-09, accuracy 0.826212\n",
            "Lambda 0.06309573444801933, top_k 400\n",
            "Ridge lambda 1e-09, accuracy 0.829424\n",
            "Lambda 0.039810717055349734, top_k 20\n",
            "Ridge lambda 0.001, accuracy 0.71234\n",
            "Lambda 0.039810717055349734, top_k 30\n",
            "Ridge lambda 1e-05, accuracy 0.712988\n",
            "Lambda 0.039810717055349734, top_k 40\n",
            "Ridge lambda 1e-09, accuracy 0.7455759999999999\n",
            "Lambda 0.039810717055349734, top_k 50\n",
            "Ridge lambda 1e-09, accuracy 0.75306\n",
            "Lambda 0.039810717055349734, top_k 80\n",
            "Ridge lambda 1e-09, accuracy 0.7808999999999999\n",
            "Lambda 0.039810717055349734, top_k 100\n",
            "Ridge lambda 1e-09, accuracy 0.7841400000000001\n",
            "Lambda 0.039810717055349734, top_k 150\n",
            "Ridge lambda 1e-09, accuracy 0.7978120000000001\n",
            "Lambda 0.039810717055349734, top_k 200\n",
            "Ridge lambda 1e-09, accuracy 0.8081160000000001\n",
            "Lambda 0.039810717055349734, top_k 250\n",
            "Ridge lambda 1e-09, accuracy 0.8189120000000001\n",
            "Lambda 0.039810717055349734, top_k 300\n",
            "Ridge lambda 1e-09, accuracy 0.821608\n",
            "Lambda 0.039810717055349734, top_k 350\n",
            "Ridge lambda 1e-09, accuracy 0.8221320000000001\n",
            "Lambda 0.039810717055349734, top_k 400\n",
            "Ridge lambda 1e-09, accuracy 0.825528\n",
            "Lambda 0.025118864315095794, top_k 20\n",
            "Ridge lambda 0.001, accuracy 0.703716\n",
            "Lambda 0.025118864315095794, top_k 30\n",
            "Ridge lambda 1e-09, accuracy 0.72726\n",
            "Lambda 0.025118864315095794, top_k 40\n",
            "Ridge lambda 1e-09, accuracy 0.745228\n",
            "Lambda 0.025118864315095794, top_k 50\n",
            "Ridge lambda 1e-09, accuracy 0.749024\n",
            "Lambda 0.025118864315095794, top_k 80\n",
            "Ridge lambda 1e-09, accuracy 0.775276\n",
            "Lambda 0.025118864315095794, top_k 100\n",
            "Ridge lambda 1e-09, accuracy 0.781412\n",
            "Lambda 0.025118864315095794, top_k 150\n",
            "Ridge lambda 1e-09, accuracy 0.8009919999999999\n",
            "Lambda 0.025118864315095794, top_k 200\n",
            "Ridge lambda 1e-09, accuracy 0.809704\n",
            "Lambda 0.025118864315095794, top_k 250\n",
            "Ridge lambda 1e-09, accuracy 0.8133440000000001\n",
            "Lambda 0.025118864315095794, top_k 300\n",
            "Ridge lambda 1e-09, accuracy 0.8212760000000001\n",
            "Lambda 0.025118864315095794, top_k 350\n",
            "Ridge lambda 1e-09, accuracy 0.822252\n",
            "Lambda 0.025118864315095794, top_k 400\n",
            "Ridge lambda 1e-09, accuracy 0.826212\n",
            "Lambda 0.015848931924611134, top_k 20\n",
            "Ridge lambda 1e-05, accuracy 0.7067\n",
            "Lambda 0.015848931924611134, top_k 30\n",
            "Ridge lambda 1e-06, accuracy 0.75588\n",
            "Lambda 0.015848931924611134, top_k 40\n",
            "Ridge lambda 1e-06, accuracy 0.7610879999999999\n",
            "Lambda 0.015848931924611134, top_k 50\n",
            "Ridge lambda 1e-05, accuracy 0.7612\n",
            "Lambda 0.015848931924611134, top_k 80\n",
            "Ridge lambda 1e-09, accuracy 0.7747120000000001\n",
            "Lambda 0.015848931924611134, top_k 100\n",
            "Ridge lambda 1e-08, accuracy 0.778092\n",
            "Lambda 0.015848931924611134, top_k 150\n",
            "Ridge lambda 1e-09, accuracy 0.7996679999999999\n",
            "Lambda 0.015848931924611134, top_k 200\n",
            "Ridge lambda 1e-09, accuracy 0.814004\n",
            "Lambda 0.015848931924611134, top_k 250\n",
            "Ridge lambda 1e-09, accuracy 0.770996\n",
            "Lambda 0.015848931924611134, top_k 300\n",
            "Ridge lambda 1e-09, accuracy 0.761504\n",
            "Lambda 0.015848931924611134, top_k 350\n",
            "Ridge lambda 1e-09, accuracy 0.774976\n",
            "Lambda 0.015848931924611134, top_k 400\n",
            "Ridge lambda 1e-09, accuracy 0.7819320000000001\n",
            "Lambda 0.01, top_k 20\n",
            "Ridge lambda 1e-09, accuracy 0.714836\n",
            "Lambda 0.01, top_k 30\n",
            "Ridge lambda 1e-08, accuracy 0.756544\n",
            "Lambda 0.01, top_k 40\n",
            "Ridge lambda 1e-06, accuracy 0.768372\n",
            "Lambda 0.01, top_k 50\n",
            "Ridge lambda 1e-06, accuracy 0.772208\n",
            "Lambda 0.01, top_k 80\n",
            "Ridge lambda 1e-09, accuracy 0.779508\n",
            "Lambda 0.01, top_k 100\n",
            "Ridge lambda 1e-07, accuracy 0.784316\n",
            "Lambda 0.01, top_k 150\n",
            "Ridge lambda 1e-05, accuracy 0.79066\n",
            "Lambda 0.01, top_k 200\n",
            "Ridge lambda 1e-09, accuracy 0.795024\n",
            "Lambda 0.01, top_k 250\n",
            "Ridge lambda 1e-09, accuracy 0.775452\n",
            "Lambda 0.01, top_k 300\n",
            "Ridge lambda 1e-09, accuracy 0.7655200000000001\n",
            "Lambda 0.01, top_k 350\n",
            "Ridge lambda 1e-07, accuracy 0.767736\n",
            "Lambda 0.01, top_k 400\n",
            "Ridge lambda 1e-09, accuracy 0.7779959999999999\n",
            "Lambda 0.00630957344480193, top_k 20\n",
            "Ridge lambda 1e-08, accuracy 0.7131320000000001\n",
            "Lambda 0.00630957344480193, top_k 30\n",
            "Ridge lambda 1e-09, accuracy 0.7267319999999999\n",
            "Lambda 0.00630957344480193, top_k 40\n",
            "Ridge lambda 1e-07, accuracy 0.76444\n",
            "Lambda 0.00630957344480193, top_k 50\n",
            "Ridge lambda 1e-09, accuracy 0.77788\n",
            "Lambda 0.00630957344480193, top_k 80\n",
            "Ridge lambda 1e-09, accuracy 0.781188\n",
            "Lambda 0.00630957344480193, top_k 100\n",
            "Ridge lambda 1e-09, accuracy 0.7833439999999999\n",
            "Lambda 0.00630957344480193, top_k 150\n",
            "Ridge lambda 1e-09, accuracy 0.7919600000000001\n",
            "Lambda 0.00630957344480193, top_k 200\n",
            "Ridge lambda 1e-09, accuracy 0.801956\n",
            "Lambda 0.00630957344480193, top_k 250\n",
            "Ridge lambda 1e-09, accuracy 0.80532\n",
            "Lambda 0.00630957344480193, top_k 300\n",
            "Ridge lambda 1e-09, accuracy 0.786644\n",
            "Lambda 0.00630957344480193, top_k 350\n",
            "Ridge lambda 1e-09, accuracy 0.7707999999999999\n",
            "Lambda 0.00630957344480193, top_k 400\n",
            "Ridge lambda 1e-05, accuracy 0.7737\n",
            "Lambda 0.003981071705534973, top_k 20\n",
            "Ridge lambda 1e-09, accuracy 0.719392\n",
            "Lambda 0.003981071705534973, top_k 30\n",
            "Ridge lambda 1e-09, accuracy 0.736284\n",
            "Lambda 0.003981071705534973, top_k 40\n",
            "Ridge lambda 1e-06, accuracy 0.7467680000000001\n",
            "Lambda 0.003981071705534973, top_k 50\n",
            "Ridge lambda 1e-08, accuracy 0.768036\n",
            "Lambda 0.003981071705534973, top_k 80\n",
            "Ridge lambda 1e-09, accuracy 0.788052\n",
            "Lambda 0.003981071705534973, top_k 100\n",
            "Ridge lambda 1e-09, accuracy 0.79022\n",
            "Lambda 0.003981071705534973, top_k 150\n",
            "Ridge lambda 1e-09, accuracy 0.812388\n",
            "Lambda 0.003981071705534973, top_k 200\n",
            "Ridge lambda 1e-09, accuracy 0.8159519999999999\n",
            "Lambda 0.003981071705534973, top_k 250\n",
            "Ridge lambda 1e-09, accuracy 0.8236920000000001\n",
            "Lambda 0.003981071705534973, top_k 300\n",
            "Ridge lambda 0.0001, accuracy 0.823076\n",
            "Lambda 0.003981071705534973, top_k 350\n",
            "Ridge lambda 1e-09, accuracy 0.8291359999999999\n",
            "Lambda 0.003981071705534973, top_k 400\n",
            "Ridge lambda 1e-09, accuracy 0.829988\n",
            "Lambda 0.0025118864315095794, top_k 20\n",
            "Ridge lambda 1e-09, accuracy 0.7212000000000001\n",
            "Lambda 0.0025118864315095794, top_k 30\n",
            "Ridge lambda 1e-09, accuracy 0.7397720000000001\n",
            "Lambda 0.0025118864315095794, top_k 40\n",
            "Ridge lambda 1e-08, accuracy 0.7573719999999999\n",
            "Lambda 0.0025118864315095794, top_k 50\n",
            "Ridge lambda 1e-09, accuracy 0.76522\n",
            "Lambda 0.0025118864315095794, top_k 80\n",
            "Ridge lambda 1e-09, accuracy 0.782672\n",
            "Lambda 0.0025118864315095794, top_k 100\n",
            "Ridge lambda 1e-09, accuracy 0.7907799999999999\n",
            "Lambda 0.0025118864315095794, top_k 150\n",
            "Ridge lambda 1e-09, accuracy 0.8061919999999999\n",
            "Lambda 0.0025118864315095794, top_k 200\n",
            "Ridge lambda 1e-09, accuracy 0.819524\n",
            "Lambda 0.0025118864315095794, top_k 250\n",
            "Ridge lambda 0.0001, accuracy 0.8199720000000001\n",
            "Lambda 0.0025118864315095794, top_k 300\n",
            "Ridge lambda 1e-09, accuracy 0.8211999999999999\n",
            "Lambda 0.0025118864315095794, top_k 350\n",
            "Ridge lambda 1e-09, accuracy 0.828708\n",
            "Lambda 0.0025118864315095794, top_k 400\n",
            "Ridge lambda 1e-09, accuracy 0.8294600000000001\n",
            "Lambda 0.001584893192461114, top_k 20\n",
            "Ridge lambda 1e-07, accuracy 0.731024\n",
            "Lambda 0.001584893192461114, top_k 30\n",
            "Ridge lambda 1e-07, accuracy 0.743952\n",
            "Lambda 0.001584893192461114, top_k 40\n",
            "Ridge lambda 1e-09, accuracy 0.7529800000000001\n",
            "Lambda 0.001584893192461114, top_k 50\n",
            "Ridge lambda 1e-07, accuracy 0.7652720000000001\n",
            "Lambda 0.001584893192461114, top_k 80\n",
            "Ridge lambda 1e-09, accuracy 0.78584\n",
            "Lambda 0.001584893192461114, top_k 100\n",
            "Ridge lambda 1e-09, accuracy 0.78868\n",
            "Lambda 0.001584893192461114, top_k 150\n",
            "Ridge lambda 1e-09, accuracy 0.80542\n",
            "Lambda 0.001584893192461114, top_k 200\n",
            "Ridge lambda 1e-09, accuracy 0.814752\n",
            "Lambda 0.001584893192461114, top_k 250\n",
            "Ridge lambda 1e-09, accuracy 0.823724\n",
            "Lambda 0.001584893192461114, top_k 300\n",
            "Ridge lambda 1e-08, accuracy 0.824944\n",
            "Lambda 0.001584893192461114, top_k 350\n",
            "Ridge lambda 1e-09, accuracy 0.8268559999999999\n",
            "Lambda 0.001584893192461114, top_k 400\n",
            "Ridge lambda 1e-09, accuracy 0.8286520000000002\n",
            "Lambda 0.001, top_k 20\n",
            "Ridge lambda 1e-09, accuracy 0.732324\n",
            "Lambda 0.001, top_k 30\n",
            "Ridge lambda 1e-09, accuracy 0.745124\n",
            "Lambda 0.001, top_k 40\n",
            "Ridge lambda 1e-08, accuracy 0.7679800000000001\n",
            "Lambda 0.001, top_k 50\n",
            "Ridge lambda 1e-09, accuracy 0.769344\n",
            "Lambda 0.001, top_k 80\n",
            "Ridge lambda 1e-09, accuracy 0.7887799999999999\n",
            "Lambda 0.001, top_k 100\n",
            "Ridge lambda 1e-09, accuracy 0.8059160000000001\n",
            "Lambda 0.001, top_k 150\n",
            "Ridge lambda 1e-08, accuracy 0.8124560000000001\n",
            "Lambda 0.001, top_k 200\n",
            "Ridge lambda 0.0001, accuracy 0.8210720000000002\n",
            "Lambda 0.001, top_k 250\n",
            "Ridge lambda 1e-09, accuracy 0.819252\n",
            "Lambda 0.001, top_k 300\n",
            "Ridge lambda 0.001, accuracy 0.7932\n",
            "Lambda 0.001, top_k 350\n",
            "Ridge lambda 0.0001, accuracy 0.7974439999999999\n",
            "Lambda 0.001, top_k 400\n",
            "Ridge lambda 1e-05, accuracy 0.7939\n",
            "Lambda 0.000630957344480193, top_k 20\n",
            "Ridge lambda 1e-09, accuracy 0.738452\n",
            "Lambda 0.000630957344480193, top_k 30\n",
            "Ridge lambda 1e-09, accuracy 0.762984\n",
            "Lambda 0.000630957344480193, top_k 40\n",
            "Ridge lambda 1e-07, accuracy 0.765172\n",
            "Lambda 0.000630957344480193, top_k 50\n",
            "Ridge lambda 1e-06, accuracy 0.7706799999999999\n",
            "Lambda 0.000630957344480193, top_k 80\n",
            "Ridge lambda 1e-09, accuracy 0.790796\n",
            "Lambda 0.000630957344480193, top_k 100\n",
            "Ridge lambda 1e-09, accuracy 0.798672\n",
            "Lambda 0.000630957344480193, top_k 150\n",
            "Ridge lambda 1e-09, accuracy 0.8126520000000002\n",
            "Lambda 0.000630957344480193, top_k 200\n",
            "Ridge lambda 1e-09, accuracy 0.8187599999999999\n",
            "Lambda 0.000630957344480193, top_k 250\n",
            "Ridge lambda 1e-09, accuracy 0.823164\n",
            "Lambda 0.000630957344480193, top_k 300\n",
            "Ridge lambda 1e-08, accuracy 0.82578\n",
            "Lambda 0.000630957344480193, top_k 350\n",
            "Ridge lambda 1e-09, accuracy 0.8278880000000001\n",
            "Lambda 0.000630957344480193, top_k 400\n",
            "Ridge lambda 1e-09, accuracy 0.828908\n",
            "Lambda 0.00039810717055349735, top_k 20\n",
            "Ridge lambda 1e-08, accuracy 0.734796\n",
            "Lambda 0.00039810717055349735, top_k 30\n",
            "Ridge lambda 1e-05, accuracy 0.7512040000000001\n",
            "Lambda 0.00039810717055349735, top_k 40\n",
            "Ridge lambda 1e-06, accuracy 0.7532880000000001\n",
            "Lambda 0.00039810717055349735, top_k 50\n",
            "Ridge lambda 1e-08, accuracy 0.769164\n",
            "Lambda 0.00039810717055349735, top_k 80\n",
            "Ridge lambda 1e-09, accuracy 0.793904\n",
            "Lambda 0.00039810717055349735, top_k 100\n",
            "Ridge lambda 1e-09, accuracy 0.7982480000000001\n",
            "Lambda 0.00039810717055349735, top_k 150\n",
            "Ridge lambda 1e-09, accuracy 0.8131400000000001\n",
            "Lambda 0.00039810717055349735, top_k 200\n",
            "Ridge lambda 1e-09, accuracy 0.817632\n",
            "Lambda 0.00039810717055349735, top_k 250\n",
            "Ridge lambda 1e-09, accuracy 0.8197720000000001\n",
            "Lambda 0.00039810717055349735, top_k 300\n",
            "Ridge lambda 1e-09, accuracy 0.8244960000000001\n",
            "Lambda 0.00039810717055349735, top_k 350\n",
            "Ridge lambda 1e-09, accuracy 0.828444\n",
            "Lambda 0.00039810717055349735, top_k 400\n",
            "Ridge lambda 1e-09, accuracy 0.828924\n",
            "Lambda 0.00025118864315095795, top_k 20\n",
            "Ridge lambda 1e-07, accuracy 0.72958\n",
            "Lambda 0.00025118864315095795, top_k 30\n",
            "Ridge lambda 1e-07, accuracy 0.737716\n",
            "Lambda 0.00025118864315095795, top_k 40\n",
            "Ridge lambda 1e-09, accuracy 0.7562\n",
            "Lambda 0.00025118864315095795, top_k 50\n",
            "Ridge lambda 1e-07, accuracy 0.76712\n",
            "Lambda 0.00025118864315095795, top_k 80\n",
            "Ridge lambda 1e-09, accuracy 0.7954359999999999\n",
            "Lambda 0.00025118864315095795, top_k 100\n",
            "Ridge lambda 1e-08, accuracy 0.8031880000000001\n",
            "Lambda 0.00025118864315095795, top_k 150\n",
            "Ridge lambda 1e-06, accuracy 0.815188\n",
            "Lambda 0.00025118864315095795, top_k 200\n",
            "Ridge lambda 1e-09, accuracy 0.8190480000000001\n",
            "Lambda 0.00025118864315095795, top_k 250\n",
            "Ridge lambda 1e-09, accuracy 0.8244999999999999\n",
            "Lambda 0.00025118864315095795, top_k 300\n",
            "Ridge lambda 1e-09, accuracy 0.8261800000000001\n",
            "Lambda 0.00025118864315095795, top_k 350\n",
            "Ridge lambda 1e-09, accuracy 0.8276239999999999\n",
            "Lambda 0.00025118864315095795, top_k 400\n",
            "Ridge lambda 1e-05, accuracy 0.8253959999999999\n",
            "Lambda 0.00015848931924611142, top_k 20\n",
            "Ridge lambda 1e-07, accuracy 0.729232\n",
            "Lambda 0.00015848931924611142, top_k 30\n",
            "Ridge lambda 1e-07, accuracy 0.7375679999999999\n",
            "Lambda 0.00015848931924611142, top_k 40\n",
            "Ridge lambda 1e-09, accuracy 0.759412\n",
            "Lambda 0.00015848931924611142, top_k 50\n",
            "Ridge lambda 1e-08, accuracy 0.770164\n",
            "Lambda 0.00015848931924611142, top_k 80\n",
            "Ridge lambda 1e-09, accuracy 0.794092\n",
            "Lambda 0.00015848931924611142, top_k 100\n",
            "Ridge lambda 1e-09, accuracy 0.8024319999999999\n",
            "Lambda 0.00015848931924611142, top_k 150\n",
            "Ridge lambda 1e-09, accuracy 0.816548\n",
            "Lambda 0.00015848931924611142, top_k 200\n",
            "Ridge lambda 1e-09, accuracy 0.8205959999999999\n",
            "Lambda 0.00015848931924611142, top_k 250\n",
            "Ridge lambda 1e-09, accuracy 0.8235679999999999\n",
            "Lambda 0.00015848931924611142, top_k 300\n",
            "Ridge lambda 1e-06, accuracy 0.827224\n",
            "Lambda 0.00015848931924611142, top_k 350\n",
            "Ridge lambda 1e-09, accuracy 0.8291359999999999\n",
            "Lambda 0.00015848931924611142, top_k 400\n",
            "Ridge lambda 1e-09, accuracy 0.8286519999999999\n",
            "Lambda 0.0001, top_k 20\n",
            "Ridge lambda 1e-06, accuracy 0.72864\n",
            "Lambda 0.0001, top_k 30\n",
            "Ridge lambda 1e-05, accuracy 0.735748\n",
            "Lambda 0.0001, top_k 40\n",
            "Ridge lambda 1e-09, accuracy 0.759412\n",
            "Lambda 0.0001, top_k 50\n",
            "Ridge lambda 1e-09, accuracy 0.7723439999999999\n",
            "Lambda 0.0001, top_k 80\n",
            "Ridge lambda 1e-09, accuracy 0.792396\n",
            "Lambda 0.0001, top_k 100\n",
            "Ridge lambda 1e-09, accuracy 0.801352\n",
            "Lambda 0.0001, top_k 150\n",
            "Ridge lambda 1e-09, accuracy 0.818704\n",
            "Lambda 0.0001, top_k 200\n",
            "Ridge lambda 1e-09, accuracy 0.8249639999999999\n",
            "Lambda 0.0001, top_k 250\n",
            "Ridge lambda 1e-09, accuracy 0.826764\n",
            "Lambda 0.0001, top_k 300\n",
            "Ridge lambda 1e-08, accuracy 0.8280480000000001\n",
            "Lambda 0.0001, top_k 350\n",
            "Ridge lambda 1e-09, accuracy 0.8293519999999999\n",
            "Lambda 0.0001, top_k 400\n",
            "Ridge lambda 1e-09, accuracy 0.8285199999999999\n",
            "Lambda 6.309573444801929e-05, top_k 20\n",
            "Ridge lambda 1e-07, accuracy 0.72854\n",
            "Lambda 6.309573444801929e-05, top_k 30\n",
            "Ridge lambda 1e-07, accuracy 0.737684\n",
            "Lambda 6.309573444801929e-05, top_k 40\n",
            "Ridge lambda 1e-08, accuracy 0.7603160000000001\n",
            "Lambda 6.309573444801929e-05, top_k 50\n",
            "Ridge lambda 1e-07, accuracy 0.7728680000000001\n",
            "Lambda 6.309573444801929e-05, top_k 80\n",
            "Ridge lambda 1e-09, accuracy 0.7944440000000002\n",
            "Lambda 6.309573444801929e-05, top_k 100\n",
            "Ridge lambda 1e-08, accuracy 0.801408\n",
            "Lambda 6.309573444801929e-05, top_k 150\n",
            "Ridge lambda 1e-09, accuracy 0.81836\n",
            "Lambda 6.309573444801929e-05, top_k 200\n",
            "Ridge lambda 1e-09, accuracy 0.825808\n",
            "Lambda 6.309573444801929e-05, top_k 250\n",
            "Ridge lambda 1e-05, accuracy 0.82456\n",
            "Lambda 6.309573444801929e-05, top_k 300\n",
            "Ridge lambda 1e-09, accuracy 0.828392\n",
            "Lambda 6.309573444801929e-05, top_k 350\n",
            "Ridge lambda 1e-09, accuracy 0.828496\n",
            "Lambda 6.309573444801929e-05, top_k 400\n",
            "Ridge lambda 1e-09, accuracy 0.828228\n",
            "Lambda 3.9810717055349695e-05, top_k 20\n",
            "Ridge lambda 1e-07, accuracy 0.72854\n",
            "Lambda 3.9810717055349695e-05, top_k 30\n",
            "Ridge lambda 1e-07, accuracy 0.737684\n",
            "Lambda 3.9810717055349695e-05, top_k 40\n",
            "Ridge lambda 1e-08, accuracy 0.7603160000000001\n",
            "Lambda 3.9810717055349695e-05, top_k 50\n",
            "Ridge lambda 1e-09, accuracy 0.7725519999999999\n",
            "Lambda 3.9810717055349695e-05, top_k 80\n",
            "Ridge lambda 1e-09, accuracy 0.7944440000000002\n",
            "Lambda 3.9810717055349695e-05, top_k 100\n",
            "Ridge lambda 1e-09, accuracy 0.803608\n",
            "Lambda 3.9810717055349695e-05, top_k 150\n",
            "Ridge lambda 1e-09, accuracy 0.819936\n",
            "Lambda 3.9810717055349695e-05, top_k 200\n",
            "Ridge lambda 1e-09, accuracy 0.825492\n",
            "Lambda 3.9810717055349695e-05, top_k 250\n",
            "Ridge lambda 1e-09, accuracy 0.827272\n",
            "Lambda 3.9810717055349695e-05, top_k 300\n",
            "Ridge lambda 1e-09, accuracy 0.828444\n",
            "Lambda 3.9810717055349695e-05, top_k 350\n",
            "Ridge lambda 1e-08, accuracy 0.8279799999999999\n",
            "Lambda 3.9810717055349695e-05, top_k 400\n",
            "Ridge lambda 1e-09, accuracy 0.8280200000000001\n",
            "Lambda 2.5118864315095822e-05, top_k 20\n",
            "Ridge lambda 1e-07, accuracy 0.737036\n",
            "Lambda 2.5118864315095822e-05, top_k 30\n",
            "Ridge lambda 1e-08, accuracy 0.75966\n",
            "Lambda 2.5118864315095822e-05, top_k 40\n",
            "Ridge lambda 1e-09, accuracy 0.771104\n",
            "Lambda 2.5118864315095822e-05, top_k 50\n",
            "Ridge lambda 1e-08, accuracy 0.781128\n",
            "Lambda 2.5118864315095822e-05, top_k 80\n",
            "Ridge lambda 1e-09, accuracy 0.7987599999999999\n",
            "Lambda 2.5118864315095822e-05, top_k 100\n",
            "Ridge lambda 1e-09, accuracy 0.811892\n",
            "Lambda 2.5118864315095822e-05, top_k 150\n",
            "Ridge lambda 1e-09, accuracy 0.822756\n",
            "Lambda 2.5118864315095822e-05, top_k 200\n",
            "Ridge lambda 1e-09, accuracy 0.82602\n",
            "Lambda 2.5118864315095822e-05, top_k 250\n",
            "Ridge lambda 1e-09, accuracy 0.82768\n",
            "Lambda 2.5118864315095822e-05, top_k 300\n",
            "Ridge lambda 1e-09, accuracy 0.8291919999999999\n",
            "Lambda 2.5118864315095822e-05, top_k 350\n",
            "Ridge lambda 1e-09, accuracy 0.829156\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20OcXjqyU3JR",
        "outputId": "dc437529-a483-40d6-89d5-64bf444b988d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import sys\n",
        "import numpy\n",
        "import time\n",
        "numpy.set_printoptions(threshold=sys.maxsize)\n",
        "\n",
        "for i in range(23, 32):\n",
        "  lasso_lambda_ = 10 ** (-i / 5)\n",
        "  time.sleep(1) # To clear RAM if needed\n",
        "  lasso_w, _ = lasso_logistic_regression_sgd(y_train, tx_train_2, lasso_lambda_, np.zeros(tx_train_2.shape[1]), 5, 500, .3)\n",
        "  for k_subset in [20, 30, 40, 50, 80, 100, 150, 200, 250, 300, 350, 400]:\n",
        "    time.sleep(1)\n",
        "    top_k_w = get_top_abs_k_indexes(lasso_w, k_subset)\n",
        "\n",
        "    features_subset = tx_train_2[:, top_k_w]\n",
        "\n",
        "    print(f'Lambda {lasso_lambda_}, top_k {k_subset}')\n",
        "    best_ridge_lambda, mean_acc = tune_lambda(\n",
        "        y_train, features_subset, [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9], history=False)\n",
        "    print(f'Ridge lambda {best_ridge_lambda}, accuracy {mean_acc}')\n",
        "    with open(f'{DATA_FILE_PREFIX}lasso_data_no_abs_granular_no_plus.txt', 'a') as f:\n",
        "      list_top_k = list(top_k_w)\n",
        "      print(f'lambda {lasso_lambda_}, k {k_subset}: accuracy {mean_acc}, best ridge lambda {best_ridge_lambda}, top features \\n{list_top_k}\\n\\n\\n', file=f)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/implementations.py:197: RuntimeWarning: overflow encountered in exp\n",
            "  exp_x = np.exp(x)\n",
            "/content/implementations.py:200: RuntimeWarning: invalid value encountered in true_divide\n",
            "  exp_x / (1 + exp_x))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Lambda 2.5118864315095822e-05, top_k 20\n",
            "Ridge lambda 1e-07, accuracy 0.736272\n",
            "Lambda 2.5118864315095822e-05, top_k 30\n",
            "Ridge lambda 1e-07, accuracy 0.762408\n",
            "Lambda 2.5118864315095822e-05, top_k 40\n",
            "Ridge lambda 1e-07, accuracy 0.7724200000000001\n",
            "Lambda 2.5118864315095822e-05, top_k 50\n",
            "Ridge lambda 1e-09, accuracy 0.778608\n",
            "Lambda 2.5118864315095822e-05, top_k 80\n",
            "Ridge lambda 1e-08, accuracy 0.806532\n",
            "Lambda 2.5118864315095822e-05, top_k 100\n",
            "Ridge lambda 1e-07, accuracy 0.812176\n",
            "Lambda 2.5118864315095822e-05, top_k 150\n",
            "Ridge lambda 1e-09, accuracy 0.8219879999999999\n",
            "Lambda 2.5118864315095822e-05, top_k 200\n",
            "Ridge lambda 1e-09, accuracy 0.82584\n",
            "Lambda 2.5118864315095822e-05, top_k 250\n",
            "Ridge lambda 1e-09, accuracy 0.8282320000000001\n",
            "Lambda 2.5118864315095822e-05, top_k 300\n",
            "Ridge lambda 1e-09, accuracy 0.8296240000000001\n",
            "Lambda 2.5118864315095822e-05, top_k 350\n",
            "Ridge lambda 1e-05, accuracy 0.8269719999999999\n",
            "Lambda 2.5118864315095822e-05, top_k 400\n",
            "Ridge lambda 1e-09, accuracy 0.82684\n",
            "Lambda 1.584893192461114e-05, top_k 20\n",
            "Ridge lambda 1e-07, accuracy 0.737036\n",
            "Lambda 1.584893192461114e-05, top_k 30\n",
            "Ridge lambda 1e-06, accuracy 0.7617\n",
            "Lambda 1.584893192461114e-05, top_k 40\n",
            "Ridge lambda 1e-08, accuracy 0.769916\n",
            "Lambda 1.584893192461114e-05, top_k 50\n",
            "Ridge lambda 1e-09, accuracy 0.7809760000000001\n",
            "Lambda 1.584893192461114e-05, top_k 80\n",
            "Ridge lambda 1e-08, accuracy 0.796412\n",
            "Lambda 1.584893192461114e-05, top_k 100\n",
            "Ridge lambda 1e-09, accuracy 0.811892\n",
            "Lambda 1.584893192461114e-05, top_k 150\n",
            "Ridge lambda 1e-09, accuracy 0.8228759999999999\n",
            "Lambda 1.584893192461114e-05, top_k 200\n",
            "Ridge lambda 1e-09, accuracy 0.8258639999999999\n",
            "Lambda 1.584893192461114e-05, top_k 250\n",
            "Ridge lambda 1e-09, accuracy 0.827812\n",
            "Lambda 1.584893192461114e-05, top_k 300\n",
            "Ridge lambda 1e-09, accuracy 0.8290880000000002\n",
            "Lambda 1.584893192461114e-05, top_k 350\n",
            "Ridge lambda 1e-09, accuracy 0.8289560000000001\n",
            "Lambda 1.584893192461114e-05, top_k 400\n",
            "Ridge lambda 1e-09, accuracy 0.828928\n",
            "Lambda 1e-05, top_k 20\n",
            "Ridge lambda 1e-07, accuracy 0.737036\n",
            "Lambda 1e-05, top_k 30\n",
            "Ridge lambda 1e-07, accuracy 0.762408\n",
            "Lambda 1e-05, top_k 40\n",
            "Ridge lambda 1e-08, accuracy 0.769916\n",
            "Lambda 1e-05, top_k 50\n",
            "Ridge lambda 1e-09, accuracy 0.7809760000000001\n",
            "Lambda 1e-05, top_k 80\n",
            "Ridge lambda 1e-09, accuracy 0.795884\n",
            "Lambda 1e-05, top_k 100\n",
            "Ridge lambda 1e-09, accuracy 0.8118400000000001\n",
            "Lambda 1e-05, top_k 150\n",
            "Ridge lambda 1e-09, accuracy 0.823212\n",
            "Lambda 1e-05, top_k 200\n",
            "Ridge lambda 1e-09, accuracy 0.82592\n",
            "Lambda 1e-05, top_k 250\n",
            "Ridge lambda 1e-09, accuracy 0.827812\n",
            "Lambda 1e-05, top_k 300\n",
            "Ridge lambda 1e-09, accuracy 0.8290599999999999\n",
            "Lambda 1e-05, top_k 350\n",
            "Ridge lambda 1e-09, accuracy 0.8292999999999999\n",
            "Lambda 1e-05, top_k 400\n",
            "Ridge lambda 1e-09, accuracy 0.82818\n",
            "Lambda 6.30957344480193e-06, top_k 20\n",
            "Ridge lambda 1e-07, accuracy 0.7354879999999999\n",
            "Lambda 6.30957344480193e-06, top_k 30\n",
            "Ridge lambda 1e-07, accuracy 0.762408\n",
            "Lambda 6.30957344480193e-06, top_k 40\n",
            "Ridge lambda 1e-08, accuracy 0.769916\n",
            "Lambda 6.30957344480193e-06, top_k 50\n",
            "Ridge lambda 1e-09, accuracy 0.7809760000000001\n",
            "Lambda 6.30957344480193e-06, top_k 80\n",
            "Ridge lambda 1e-09, accuracy 0.795884\n",
            "Lambda 6.30957344480193e-06, top_k 100\n",
            "Ridge lambda 1e-09, accuracy 0.8118400000000001\n",
            "Lambda 6.30957344480193e-06, top_k 150\n",
            "Ridge lambda 1e-09, accuracy 0.823396\n",
            "Lambda 6.30957344480193e-06, top_k 200\n",
            "Ridge lambda 1e-09, accuracy 0.825808\n",
            "Lambda 6.30957344480193e-06, top_k 250\n",
            "Ridge lambda 1e-09, accuracy 0.827712\n",
            "Lambda 6.30957344480193e-06, top_k 300\n",
            "Ridge lambda 1e-09, accuracy 0.8291039999999998\n",
            "Lambda 6.30957344480193e-06, top_k 350\n",
            "Ridge lambda 1e-09, accuracy 0.829176\n",
            "Lambda 6.30957344480193e-06, top_k 400\n",
            "Ridge lambda 1e-09, accuracy 0.82818\n",
            "Lambda 3.981071705534969e-06, top_k 20\n",
            "Ridge lambda 1e-07, accuracy 0.7354879999999999\n",
            "Lambda 3.981071705534969e-06, top_k 30\n",
            "Ridge lambda 1e-07, accuracy 0.762408\n",
            "Lambda 3.981071705534969e-06, top_k 40\n",
            "Ridge lambda 1e-08, accuracy 0.769916\n",
            "Lambda 3.981071705534969e-06, top_k 50\n",
            "Ridge lambda 1e-09, accuracy 0.7809760000000001\n",
            "Lambda 3.981071705534969e-06, top_k 80\n",
            "Ridge lambda 1e-09, accuracy 0.795884\n",
            "Lambda 3.981071705534969e-06, top_k 100\n",
            "Ridge lambda 1e-09, accuracy 0.8118400000000001\n",
            "Lambda 3.981071705534969e-06, top_k 150\n",
            "Ridge lambda 1e-09, accuracy 0.823396\n",
            "Lambda 3.981071705534969e-06, top_k 200\n",
            "Ridge lambda 1e-09, accuracy 0.825808\n",
            "Lambda 3.981071705534969e-06, top_k 250\n",
            "Ridge lambda 1e-09, accuracy 0.827712\n",
            "Lambda 3.981071705534969e-06, top_k 300\n",
            "Ridge lambda 1e-08, accuracy 0.829032\n",
            "Lambda 3.981071705534969e-06, top_k 350\n",
            "Ridge lambda 1e-09, accuracy 0.8292440000000001\n",
            "Lambda 3.981071705534969e-06, top_k 400\n",
            "Ridge lambda 1e-09, accuracy 0.82818\n",
            "Lambda 2.5118864315095823e-06, top_k 20\n",
            "Ridge lambda 1e-07, accuracy 0.7354879999999999\n",
            "Lambda 2.5118864315095823e-06, top_k 30\n",
            "Ridge lambda 1e-08, accuracy 0.7614759999999998\n",
            "Lambda 2.5118864315095823e-06, top_k 40\n",
            "Ridge lambda 1e-09, accuracy 0.778176\n",
            "Lambda 2.5118864315095823e-06, top_k 50\n",
            "Ridge lambda 1e-09, accuracy 0.7809760000000001\n",
            "Lambda 2.5118864315095823e-06, top_k 80\n",
            "Ridge lambda 1e-09, accuracy 0.795884\n",
            "Lambda 2.5118864315095823e-06, top_k 100\n",
            "Ridge lambda 1e-09, accuracy 0.8118400000000001\n",
            "Lambda 2.5118864315095823e-06, top_k 150\n",
            "Ridge lambda 1e-09, accuracy 0.823396\n",
            "Lambda 2.5118864315095823e-06, top_k 200\n",
            "Ridge lambda 1e-09, accuracy 0.82568\n",
            "Lambda 2.5118864315095823e-06, top_k 250\n",
            "Ridge lambda 1e-09, accuracy 0.827712\n",
            "Lambda 2.5118864315095823e-06, top_k 300\n",
            "Ridge lambda 1e-08, accuracy 0.829032\n",
            "Lambda 2.5118864315095823e-06, top_k 350\n",
            "Ridge lambda 1e-09, accuracy 0.8292440000000001\n",
            "Lambda 2.5118864315095823e-06, top_k 400\n",
            "Ridge lambda 1e-09, accuracy 0.828032\n",
            "Lambda 1.584893192461114e-06, top_k 20\n",
            "Ridge lambda 1e-07, accuracy 0.7354879999999999\n",
            "Lambda 1.584893192461114e-06, top_k 30\n",
            "Ridge lambda 1e-08, accuracy 0.7614759999999998\n",
            "Lambda 1.584893192461114e-06, top_k 40\n",
            "Ridge lambda 1e-09, accuracy 0.778176\n",
            "Lambda 1.584893192461114e-06, top_k 50\n",
            "Ridge lambda 1e-09, accuracy 0.7809760000000001\n",
            "Lambda 1.584893192461114e-06, top_k 80\n",
            "Ridge lambda 1e-09, accuracy 0.795884\n",
            "Lambda 1.584893192461114e-06, top_k 100\n",
            "Ridge lambda 1e-09, accuracy 0.8118400000000001\n",
            "Lambda 1.584893192461114e-06, top_k 150\n",
            "Ridge lambda 1e-09, accuracy 0.823396\n",
            "Lambda 1.584893192461114e-06, top_k 200\n",
            "Ridge lambda 1e-09, accuracy 0.82568\n",
            "Lambda 1.584893192461114e-06, top_k 250\n",
            "Ridge lambda 1e-09, accuracy 0.827712\n",
            "Lambda 1.584893192461114e-06, top_k 300\n",
            "Ridge lambda 1e-08, accuracy 0.829032\n",
            "Lambda 1.584893192461114e-06, top_k 350\n",
            "Ridge lambda 1e-09, accuracy 0.8292759999999999\n",
            "Lambda 1.584893192461114e-06, top_k 400\n",
            "Ridge lambda 1e-09, accuracy 0.828032\n",
            "Lambda 1e-06, top_k 20\n",
            "Ridge lambda 1e-07, accuracy 0.7354879999999999\n",
            "Lambda 1e-06, top_k 30\n",
            "Ridge lambda 1e-08, accuracy 0.7614759999999998\n",
            "Lambda 1e-06, top_k 40\n",
            "Ridge lambda 1e-09, accuracy 0.778176\n",
            "Lambda 1e-06, top_k 50\n",
            "Ridge lambda 1e-09, accuracy 0.7809760000000001\n",
            "Lambda 1e-06, top_k 80\n",
            "Ridge lambda 1e-09, accuracy 0.795884\n",
            "Lambda 1e-06, top_k 100\n",
            "Ridge lambda 1e-09, accuracy 0.8118400000000001\n",
            "Lambda 1e-06, top_k 150\n",
            "Ridge lambda 1e-09, accuracy 0.823396\n",
            "Lambda 1e-06, top_k 200\n",
            "Ridge lambda 1e-09, accuracy 0.82568\n",
            "Lambda 1e-06, top_k 250\n",
            "Ridge lambda 1e-09, accuracy 0.827712\n",
            "Lambda 1e-06, top_k 300\n",
            "Ridge lambda 1e-08, accuracy 0.829032\n",
            "Lambda 1e-06, top_k 350\n",
            "Ridge lambda 1e-09, accuracy 0.8292759999999999\n",
            "Lambda 1e-06, top_k 400\n",
            "Ridge lambda 1e-09, accuracy 0.828032\n",
            "Lambda 6.30957344480193e-07, top_k 20\n",
            "Ridge lambda 1e-07, accuracy 0.7354879999999999\n",
            "Lambda 6.30957344480193e-07, top_k 30\n",
            "Ridge lambda 1e-08, accuracy 0.7614759999999998\n",
            "Lambda 6.30957344480193e-07, top_k 40\n",
            "Ridge lambda 1e-09, accuracy 0.778176\n",
            "Lambda 6.30957344480193e-07, top_k 50\n",
            "Ridge lambda 1e-09, accuracy 0.7809760000000001\n",
            "Lambda 6.30957344480193e-07, top_k 80\n",
            "Ridge lambda 1e-09, accuracy 0.795884\n",
            "Lambda 6.30957344480193e-07, top_k 100\n",
            "Ridge lambda 1e-09, accuracy 0.8118400000000001\n",
            "Lambda 6.30957344480193e-07, top_k 150\n",
            "Ridge lambda 1e-09, accuracy 0.823396\n",
            "Lambda 6.30957344480193e-07, top_k 200\n",
            "Ridge lambda 1e-09, accuracy 0.82568\n",
            "Lambda 6.30957344480193e-07, top_k 250\n",
            "Ridge lambda 1e-09, accuracy 0.827712\n",
            "Lambda 6.30957344480193e-07, top_k 300\n",
            "Ridge lambda 1e-08, accuracy 0.829032\n",
            "Lambda 6.30957344480193e-07, top_k 350\n",
            "Ridge lambda 1e-09, accuracy 0.8292759999999999\n",
            "Lambda 6.30957344480193e-07, top_k 400\n",
            "Ridge lambda 1e-09, accuracy 0.828032\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spIov5D_FzSI",
        "outputId": "28ab114b-b733-4e04-ec82-d6e042a889a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import sys\n",
        "import numpy\n",
        "import time\n",
        "numpy.set_printoptions(threshold=sys.maxsize)\n",
        "\n",
        "for i in range(10, 30):\n",
        "  lasso_lambda_ = 10 ** (-i / 5)\n",
        "  time.sleep(1) # To clear RAM if needed\n",
        "  lasso_w, _ = lasso_logistic_regression_sgd(y_train, tx_train_4, lasso_lambda_, np.zeros(tx_train_4.shape[1]), 5, 500, .3)\n",
        "  for k_subset in [50, 60, 70, 80, 100, 120, 150, 200]:\n",
        "    time.sleep(1)\n",
        "    top_k_w = get_top_abs_k_indexes(lasso_w, k_subset)\n",
        "\n",
        "    features_subset = tx_train_4[:, top_k_w]\n",
        "\n",
        "    print(f'Lambda {lasso_lambda_}, top_k {k_subset}')\n",
        "    best_ridge_lambda, mean_acc = tune_lambda(\n",
        "        y_train, features_subset, [1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9], history=False)\n",
        "    print(f'Ridge lambda {best_ridge_lambda}, accuracy {mean_acc}')\n",
        "    with open(f'{DATA_FILE_PREFIX}lasso_data_granular_top_80_sincos_rep.txt', 'a') as f:\n",
        "      list_top_k = list(top_k_w)\n",
        "      print(f'lambda {lasso_lambda_}, k {k_subset}: accuracy {mean_acc}, best ridge lambda {best_ridge_lambda}, top features \\n{list_top_k}\\n\\n\\n', file=f)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lambda 0.01, top_k 50\n",
            "Ridge lambda 1e-09, accuracy 0.82096\n",
            "Lambda 0.01, top_k 60\n",
            "Ridge lambda 1e-09, accuracy 0.822724\n",
            "Lambda 0.01, top_k 70\n",
            "Ridge lambda 1e-09, accuracy 0.8221440000000001\n",
            "Lambda 0.01, top_k 80\n",
            "Ridge lambda 1e-09, accuracy 0.823644\n",
            "Lambda 0.01, top_k 100\n",
            "Ridge lambda 1e-09, accuracy 0.821184\n",
            "Lambda 0.01, top_k 120\n",
            "Ridge lambda 1e-06, accuracy 0.7657480000000001\n",
            "Lambda 0.01, top_k 150\n",
            "Ridge lambda 1e-06, accuracy 0.8041079999999999\n",
            "Lambda 0.01, top_k 200\n",
            "Ridge lambda 1e-09, accuracy 0.7760239999999999\n",
            "Lambda 0.00630957344480193, top_k 50\n",
            "Ridge lambda 1e-09, accuracy 0.8256799999999999\n",
            "Lambda 0.00630957344480193, top_k 60\n",
            "Ridge lambda 1e-09, accuracy 0.827108\n",
            "Lambda 0.00630957344480193, top_k 70\n",
            "Ridge lambda 1e-09, accuracy 0.8264039999999999\n",
            "Lambda 0.00630957344480193, top_k 80\n",
            "Ridge lambda 1e-07, accuracy 0.825156\n",
            "Lambda 0.00630957344480193, top_k 100\n",
            "Ridge lambda 0.0001, accuracy 0.8080359999999999\n",
            "Lambda 0.00630957344480193, top_k 120\n",
            "Ridge lambda 1e-08, accuracy 0.7704880000000001\n",
            "Lambda 0.00630957344480193, top_k 150\n",
            "Ridge lambda 0.0001, accuracy 0.78196\n",
            "Lambda 0.00630957344480193, top_k 200\n",
            "Ridge lambda 1e-07, accuracy 0.794072\n",
            "Lambda 0.003981071705534973, top_k 50\n",
            "Ridge lambda 1e-09, accuracy 0.8275919999999999\n",
            "Lambda 0.003981071705534973, top_k 60\n",
            "Ridge lambda 1e-08, accuracy 0.827684\n",
            "Lambda 0.003981071705534973, top_k 70\n",
            "Ridge lambda 1e-09, accuracy 0.8285640000000001\n",
            "Lambda 0.003981071705534973, top_k 80\n",
            "Ridge lambda 1e-07, accuracy 0.8285039999999999\n",
            "Lambda 0.003981071705534973, top_k 100\n",
            "Ridge lambda 1e-09, accuracy 0.826068\n",
            "Lambda 0.003981071705534973, top_k 120\n",
            "Ridge lambda 1e-09, accuracy 0.78926\n",
            "Lambda 0.003981071705534973, top_k 150\n",
            "Ridge lambda 0.0001, accuracy 0.776688\n",
            "Lambda 0.003981071705534973, top_k 200\n",
            "Ridge lambda 1e-05, accuracy 0.795396\n",
            "Lambda 0.0025118864315095794, top_k 50\n",
            "Ridge lambda 1e-06, accuracy 0.827032\n",
            "Lambda 0.0025118864315095794, top_k 60\n",
            "Ridge lambda 1e-09, accuracy 0.8288040000000001\n",
            "Lambda 0.0025118864315095794, top_k 70\n",
            "Ridge lambda 1e-09, accuracy 0.8295959999999999\n",
            "Lambda 0.0025118864315095794, top_k 80\n",
            "Ridge lambda 1e-09, accuracy 0.830512\n",
            "Lambda 0.0025118864315095794, top_k 100\n",
            "Ridge lambda 1e-09, accuracy 0.83056\n",
            "Lambda 0.0025118864315095794, top_k 120\n",
            "Ridge lambda 1e-05, accuracy 0.8273679999999999\n",
            "Lambda 0.0025118864315095794, top_k 150\n",
            "Ridge lambda 1e-09, accuracy 0.7881159999999999\n",
            "Lambda 0.0025118864315095794, top_k 200\n",
            "Ridge lambda 1e-07, accuracy 0.7952359999999998\n",
            "Lambda 0.001584893192461114, top_k 50\n",
            "Ridge lambda 1e-09, accuracy 0.828028\n",
            "Lambda 0.001584893192461114, top_k 60\n",
            "Ridge lambda 1e-08, accuracy 0.829712\n",
            "Lambda 0.001584893192461114, top_k 70\n",
            "Ridge lambda 1e-09, accuracy 0.830328\n",
            "Lambda 0.001584893192461114, top_k 80\n",
            "Ridge lambda 1e-09, accuracy 0.8303800000000001\n",
            "Lambda 0.001584893192461114, top_k 100\n",
            "Ridge lambda 1e-09, accuracy 0.8314\n",
            "Lambda 0.001584893192461114, top_k 120\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}